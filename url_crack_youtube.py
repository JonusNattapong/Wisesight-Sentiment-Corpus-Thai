import subprocess
import sys
import requests
import re
import time
from urllib.parse import urljoin

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á requests ‡πÅ‡∏•‡∏∞ beautifulsoup4...")
    install("requests")
    install("beautifulsoup4")
    import requests
    from bs4 import BeautifulSoup

# ‡∏ä‡πà‡∏≠‡∏á YouTube ‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏° (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°)
channels = {
    # ‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡∏ß‡∏µ‡∏´‡∏•‡∏±‡∏Å (‡πÉ‡∏ä‡πâ channelId ‡∏à‡∏£‡∏¥‡∏á)
    'one31': 'https://www.youtube.com/@one31official',
    'workpoint': 'https://www.youtube.com/@WorkpointOfficial',
    'thairath': 'https://www.youtube.com/@thairathnews',
    'ch3thailand': 'https://www.youtube.com/@Ch3Thailand',
    'ch7hd': 'https://www.youtube.com/@ch7hd',
    'amarintv': 'https://www.youtube.com/@AMARINTVHD',
    'thaipbs': 'https://www.youtube.com/@ThaiPBS',
    '9MCOT': 'https://www.youtube.com/@9MCOT',
    'nationtv22': 'https://www.youtube.com/@nationtvTH',
    'pptv36': 'https://www.youtube.com/@PPTVHD36',
    'springnews': 'https://www.youtube.com/@springnewsonline',
    'tnn16': 'https://www.youtube.com/@TNN.Online',
    'voicetv': 'https://www.youtube.com/@voicetv',
    'ch7hdnews': 'https://www.youtube.com/@ch7hdnews',
    '3PlusNews': 'https://www.youtube.com/@3PlusNews',
    'onenews31': 'https://www.youtube.com/@onenews31',
    'Luichonkhao': 'https://www.youtube.com/@Luichonkhao',
    'thaich8news': 'https://www.youtube.com/@thaich8news',
    'maleevsking': 'https://www.youtube.com/@maleevsking',
    'GUZAP': 'https://www.youtube.com/@GUZAP',
    'mheeMovie': 'https://www.youtube.com/@mheeMovie',
    'MajorGroup': 'https://www.youtube.com/@MajorGroup',
    'Longtunman': 'https://www.youtube.com/@Longtunman',
    'NetflixThailand': 'https://www.youtube.com/@NetflixThailand',
    'LUPAS_': 'https://www.youtube.com/@LUPAS_',
    'tobenumberonechannel': 'https://www.youtube.com/@tobenumberonechannel',
    'techoffside': 'https://www.youtube.com/@techoffside',
    'TheStandardNews': 'https://www.youtube.com/@TheStandardNews',
    'TheStandardWealth': 'https://www.youtube.com/@TheStandardWealth',
    '1 MILL': 'https://www.youtube.com/channel/UC9XnG68APlP7HaI6y5NU9TQ',
    'SharkTankThailandOfficial': 'https://www.youtube.com/@SharkTankThailandOfficial',
    'WoodyWorldChannel': 'https://www.youtube.com/@WoodyWorldChannel',
    'terodigital': 'https://www.youtube.com/terodigital',  # ‡∏ä‡πà‡∏≠‡∏á‡πÄ‡∏ó‡πÇ‡∏£‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•

    # ‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏£‡∏¥‡∏á
    'gmmtv': 'https://www.youtube.com/@gmmtv',
    'gmmgrammy': 'https://www.youtube.com/@GMMGrammy',
    'rsmusic': 'https://www.youtube.com/@rsmusicthailand',
    'kamikaze': 'https://www.youtube.com/@kamikaze_music',
    'spicydisc': 'https://www.youtube.com/@spicydisc',
    
    # YouTuber ‡πÅ‡∏•‡∏∞ Content Creator ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏£‡∏¥‡∏á
    'kaykai': 'https://www.youtube.com/@KaykaiSalaiderChannel',
    'guygeegee': 'https://www.youtube.com/@HYPETRAINGROUP',
    'peach': 'https://www.youtube.com/@PEACHEATLAEK',
    'timethai': 'https://www.youtube.com/@timethaiofficial',
    
    # Gaming ‡πÅ‡∏•‡∏∞ Tech ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
    'techoffside': 'https://www.youtube.com/@techoffside',
    'droidsans': 'https://www.youtube.com/@droidsans',
    
    # ‡∏Å‡∏µ‡∏¨‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡∏ó‡πå‡∏î‡∏≠‡∏£‡πå
    'trueid': 'https://www.youtube.com/@TrueIDOfficial',
    'thaifight': 'https://www.youtube.com/@ThaifightOfficial',
    'prachatai': 'https://www.youtube.com/@prachatai',
    # ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÑ‡∏•‡∏ü‡πå‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á
    'wongnai': 'https://www.youtube.com/@WongnaiOfficial',
    'khaosod': 'https://www.youtube.com/@KhaosodTV',
    'matichon': 'https://www.youtube.com/@matichontv',
    'sanook': 'https://www.youtube.com/@sanook',
    'RedremasteRed': 'https://www.youtube.com/@RedremasteRed',
    'bodyslam': 'https://www.youtube.com/@bodyslambandtv',
    'carabao': 'https://www.youtube.com/@carabaoofficial',
    'bird': 'https://www.youtube.com/@birdthongchaichannel',
    'tnn_online': 'https://www.youtube.com/@TNN.Online',
    'jokerfamily': 'https://www.youtube.com/@JOKERFAMILYOFFICIAL',
    'news1vdo': 'https://www.youtube.com/@NEWS1VDO',
    'sorrayuth': 'https://www.youtube.com/@sorrayuth9115',
    'ckfastwork': 'https://www.youtube.com/@ckfastwork',
    'theghostradio': 'https://www.youtube.com/@Theghostradio',
    'heartrock': 'https://www.youtube.com/@HEARTROCK',
    'mono29news': 'https://www.youtube.com/@MONO29NEWSTV',
    'stunnedthailand': 'https://www.youtube.com/@stunned_thailand',
    'spdnoo1': 'https://www.youtube.com/@SPDNOO1',
    'morningnewstv3': 'https://www.youtube.com/@MorningNewsTV3',
    'ejan': 'https://www.youtube.com/@Ejan',
    'epictoys': 'https://www.youtube.com/@EpicToys',
    'jackpapho': 'https://www.youtube.com/@JACKPAPHO',
    'footballquotes': 'https://www.youtube.com/@-footballquotes-',
    'bangkokbiznews': 'https://www.youtube.com/@bangkokbiznewsthailand',
    'skizztv': 'https://www.youtube.com/@SkizzTV59',
    'thumbntk': 'https://www.youtube.com/@thumbntk',
    'taibaan': 'https://www.youtube.com/@TaiBaanOfficial',
    'serngmusic': 'https://www.youtube.com/@serngmusicofficial',
    'sianstudio': 'https://www.youtube.com/@Sianstudio',
    'spin9arm': 'https://www.youtube.com/@spin9arm',
    'FastDrama': 'https://www.youtube.com/@FastDrama',
    'honekrasaeofficial': 'https://www.youtube.com/@honekrasaeofficial',
    'TheVoiceThailand': 'https://www.youtube.com/@TheVoiceThailand',
    'nuenglc': 'https://www.youtube.com/@nuenglc',
    'nickynachat': 'https://www.youtube.com/@nickynachat',
    'ohanaclip': 'https://www.youtube.com/@ohanaclip',
    'theWatcher_Documentary': 'https://www.youtube.com/@theWatcher_Documentary',
    'MissionToTheMoonMedia': 'https://www.youtube.com/@MissionToTheMoon',
    'GMM25Thailand': 'https://www.youtube.com/@GMM25Thailand',
    
}

def get_youtube_videos_from_channel(channel_url, max_videos=120):  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 120 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡πÄ‡∏¢‡∏≠‡∏∞
    """
    ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å‡∏ä‡πà‡∏≠‡∏á YouTube ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ web scraping (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        # ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á
        videos_url = channel_url + '/videos'
        print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {videos_url}")
        
        response = requests.get(videos_url, headers=headers, timeout=15)  # ‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤ timeout
        response.raise_for_status()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÉ‡∏ô HTML
        video_pattern = r'watch\?v=([a-zA-Z0-9_-]{11})'
        video_ids = re.findall(video_pattern, response.text)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô URL ‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ã‡πâ‡∏≥
        video_urls = []
        seen_ids = set()
        
        for video_id in video_ids:
            if video_id not in seen_ids and len(video_urls) < max_videos:
                video_url = f"https://www.youtube.com/watch?v={video_id}"
                video_urls.append(video_url)
                seen_ids.add(video_id)
        
        print(f"  ‚úì ‡∏û‡∏ö‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ {len(video_urls)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return video_urls
        
    except Exception as e:
        print(f"  ‚úó ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return []


def get_youtube_videos_from_api(channel_id_or_username, api_key=None, max_results=10, channel_key=None):
    """
    ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ä‡πà‡∏≠‡∏á YouTube ‡∏î‡πâ‡∏ß‡∏¢ web scraping ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ API)
    channel_id_or_username: channel id (UC...), @username, ‡∏´‡∏£‡∏∑‡∏≠ url (https://www.youtube.com/@username)
    return: list ‡∏Ç‡∏≠‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ (url)
    """
    import re
    # ‡πÅ‡∏õ‡∏•‡∏á input ‡πÄ‡∏õ‡πá‡∏ô channel id ‡∏´‡∏£‡∏∑‡∏≠ url
    channel_id = None
    channel_url = None
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô url
    if isinstance(channel_id_or_username, str) and channel_id_or_username.startswith('http'):
        m = re.search(r'youtube\.com/(?:channel/)?(UC[\w-]+)', channel_id_or_username)
        if m:
            channel_id = m.group(1)
        else:
            # ‡πÄ‡∏õ‡πá‡∏ô @handle ‡∏´‡∏£‡∏∑‡∏≠ /user/xxx
            m = re.search(r'youtube\.com/@([\w\.-]+)', channel_id_or_username)
            if m:
                handle = m.group(1)
                channel_url = f"https://www.youtube.com/@{handle}"
    elif isinstance(channel_id_or_username, str) and channel_id_or_username.startswith('UC'):
        channel_id = channel_id_or_username
    elif isinstance(channel_id_or_username, str) and channel_id_or_username.startswith('@'):
        handle = channel_id_or_username[1:]
        channel_url = f"https://www.youtube.com/@{handle}"
    else:
        # ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        print(f"[SCRAPER] ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö channel: {channel_id_or_username}")
        return []
    if channel_id:
        channel_url = f"https://www.youtube.com/channel/{channel_id}"
    if not channel_url:
        print(f"[SCRAPER] ‡πÑ‡∏°‡πà‡∏û‡∏ö url ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {channel_id_or_username}")
        return []
    return get_youtube_videos_from_channel(channel_url, max_videos=max_results)

def get_comment_count(video_url):
    """‡∏î‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ YouTube video (scrape)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        resp = requests.get(video_url, headers=headers, timeout=15)
        resp.raise_for_status()
        html = resp.text
        # ‡∏´‡∏≤ initialData JSON
        m = re.search(r'var ytInitialData = (\{.*?\});', html, re.DOTALL)
        if not m:
            m = re.search(r'window\["ytInitialData"\] = (\{.*?\});', html, re.DOTALL)
        if m:
            import json
            try:
                data = json.loads(m.group(1))
                # ‡∏´‡∏≤ commentCount ‡πÉ‡∏ô JSON
                # ‡∏≠‡∏≤‡∏à‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô videoPrimaryInfoRenderer ‡∏´‡∏£‡∏∑‡∏≠ elsewhere
                count = None
                # ‡∏ß‡∏¥‡∏ò‡∏µ 1: ‡∏´‡∏≤‡πÉ‡∏ô videoPrimaryInfoRenderer
                try:
                    count = data['contents']['twoColumnWatchNextResults']['results']['results']['contents'][0]['videoPrimaryInfoRenderer']['videoActions']['menuRenderer']['topLevelButtons'][2]['toggleButtonRenderer']['defaultText']['simpleText']
                except Exception:
                    pass
                # ‡∏ß‡∏¥‡∏ò‡∏µ 2: ‡∏´‡∏≤‡πÉ‡∏ô microformat
                if not count:
                    try:
                        count = data['microformat']['playerMicroformatRenderer']['commentCount']
                    except Exception:
                        pass
                # ‡∏ß‡∏¥‡∏ò‡∏µ 3: ‡∏´‡∏≤‡πÉ‡∏ô "commentCount" regex
                if not count:
                    m2 = re.search(r'"commentCount":\s*"?(\d+)"?', html)
                    if m2:
                        count = m2.group(1)
                if count:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô int
                    count = int(count.replace(',', '').replace('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô', '').strip())
                    return count
            except Exception:
                pass
        # fallback: ‡∏´‡∏≤ "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô" ‡πÉ‡∏ô HTML
        m = re.search(r'(\d+[\,\d]*)\s*‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô', html)
        if m:
            count = int(m.group(1).replace(',', ''))
            return count
    except Exception as e:
        print(f"[SCRAPER] Error fetching comment count for {video_url}: {e}")
    return 0

print("=" * 60)
print("üé• YouTube URL Collector ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡πÑ‡∏ó‡∏¢ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß)")
print("=" * 60)

all_links = []
successful_channels = 0
total_channels = len(channels)

# ‡πÇ‡∏´‡∏•‡∏î API KEY ‡∏à‡∏≤‡∏Å .env
import os
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
api_key = os.environ.get("YOUTUBE_API_KEY")
if not api_key or api_key == "your_api_key_here":
    print("[API] ‡πÑ‡∏°‡πà‡∏û‡∏ö YOUTUBE_API_KEY ‡πÉ‡∏ô .env ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
    exit(1)

# Update the main video-fetching loop to use scraping only, no channelId resolving
num_per_channel = 10  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡∏ä‡πà‡∏≠‡∏á (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
per_channel_links = {}
all_links = []
successful_channels = 0
total_channels = len(channels)
for i, (channel_name, channel_url) in enumerate(channels.items(), 1):
    print(f"\nüì∫ Fetching videos for: {channel_name} ({i}/{total_channels})")
    import random
    videos = get_youtube_videos_from_api(channel_url, max_results=120, channel_key=channel_name)
    if len(videos) > num_per_channel:
        top_videos = random.sample(videos, num_per_channel)
    else:
        top_videos = videos
    if top_videos:
        per_channel_links[channel_name] = top_videos
        all_links.extend(top_videos)
        successful_channels += 1
    else:
        print(f"  ‚úó [SCRAPER] No videos found or error for {channel_name}")
    if i % 5 == 0:
        print(f"\nüìä Status: Processed {i}/{total_channels} channels, successful {successful_channels}, links {len(all_links)}")

# ‡∏£‡∏ß‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á
latest_links = []
for channel_name in per_channel_links:
    latest_links.extend(per_channel_links[channel_name])

output_file = f"youtube_latest_links_{num_per_channel}per_channel.txt"
with open(output_file, "w", encoding="utf-8") as f:
    for link in latest_links:
        f.write(link + "\n")

print(f"\n‚úÖ ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î {num_per_channel} ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡πà‡∏≠‡∏ä‡πà‡∏≠‡∏á ‡∏£‡∏ß‡∏° {len(latest_links)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
print(f"üìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
print("\nüîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:")
for i, link in enumerate(latest_links[:5], 1):
    print(f"   {i}. {link}")
if len(latest_links) > 5:
    print(f"   ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(latest_links) - 5} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

print(f"\nüí° ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö get_comments.py ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ --from_file {output_file}")

if __name__ == "__main__":
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö YouTube Data API v3
    print("\n=== ‡∏ó‡∏î‡∏™‡∏≠‡∏ö YouTube Data API v3 ===")
    try:
        import os
        # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î api_key ‡∏à‡∏≤‡∏Å .env ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ python-dotenv
        api_key = os.environ.get("YOUTUBE_API_KEY")
        try:
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.environ.get("YOUTUBE_API_KEY", api_key)
        except ImportError:
            pass
        if not api_key or api_key == "your_api_key_here":
            print("[API TEST] ‡πÑ‡∏°‡πà‡∏û‡∏ö YOUTUBE_API_KEY ‡πÉ‡∏ô .env ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
        else:
            # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏î‡∏∂‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ä‡πà‡∏≠‡∏á one31official
            test_channel = "@one31official"
            print(f"[API TEST] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å {test_channel} ...")
            api_videos = get_youtube_videos_from_api(test_channel, api_key, max_results=5)
            for i, v in enumerate(api_videos, 1):
                print(f"  {i}. {v}")
            if not api_videos:
                print("[API TEST] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î")
    except Exception as e:
        print(f"[API TEST] ERROR: {e}")
