import argparse
import os
import json
import subprocess
from glob import glob
from tqdm import tqdm
# р╣Ар╕Ыр╕ер╕╡р╣Ир╕вр╕Щр╕Ир╕▓р╕Б ml_sentiment_analysis р╣Ар╕Ыр╣Зр╕Щ sentiment_integration р╕кр╕│р╕лр╕гр╕▒р╕Ър╕гр╕░р╕Ър╕Ър╣Гр╕лр╕бр╣И
try:
    from sentiment_integration import analyze_detailed_sentiment, enhanced_analyze_sentiment
    DETAILED_SENTIMENT_AVAILABLE = True
except ImportError:
    # Fallback р╣Др╕Ыр╕вр╕▒р╕Зр╕гр╕░р╕Ър╕Ър╣Ар╕Фр╕┤р╕бр╕Цр╣Йр╕▓р╣Вр╕бр╕Фр╕╣р╕ер╣Гр╕лр╕бр╣Ир╣Др╕бр╣Ир╕Юр╕гр╣Йр╕нр╕б
    try:
        from ml_sentiment_analysis import analyze_sentiment
        DETAILED_SENTIMENT_AVAILABLE = False
    except ImportError:
        def analyze_sentiment(text):
            return {"sentiment": "neutral", "confidence": 0.0, "sentiment_score": 0.0}
        DETAILED_SENTIMENT_AVAILABLE = False

import time
import random
import hashlib
import re

# === EMOTION LABEL SCHEMA ===
EMOTION_LABELS = [
    # Positive
    "р╕Фр╕╡р╣Гр╕И", "р╕Кр╕нр╕Ъ", "р╕Лр╕╢р╣Йр╕Зр╣Гр╕И", "р╕Юр╕нр╣Гр╕И", "р╕гр╕▒р╕Б",
    
    # Negative  
    "р╣Вр╕Бр╕гр╕Ш", "р╣Ар╕кр╕╡р╕вр╣Гр╕И", "р╕Ьр╕┤р╕Фр╕лр╕зр╕▒р╕З", "р╕гр╕│р╕Др╕▓р╕Н", "р╣Ар╕Бр╕ер╕╡р╕вр╕Ф", "р╕Бр╕ер╕▒р╕з", "р╕нр╕╢р╕Фр╕нр╕▒р╕Ф", "р╕Хр╕Бр╣Гр╕И",
    
    # Neutral
    "р╣Ар╕Йр╕в р╣Ж", "р╣Др╕бр╣Ир╕гр╕╣р╣Йр╕кр╕╢р╕Бр╕нр╕░р╣Др╕г", "р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Вр╣Ир╕▓р╕зр╕кр╕▓р╕г",
    
    # Others (Complex emotions)
    "р╕Ыр╕гр╕░р╕Кр╕Ф", "р╕Вр╕│р╕Вр╕▒р╕Щ", "р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡", "р╕кр╕▒р╕Ър╕кр╕Щ"
]

# Emotion grouping
EMOTION_GROUPS = {
    "Positive": ["р╕Фр╕╡р╣Гр╕И", "р╕Кр╕нр╕Ъ", "р╕Лр╕╢р╣Йр╕Зр╣Гр╕И", "р╕Юр╕нр╣Гр╕И", "р╕гр╕▒р╕Б"],
    "Negative": ["р╣Вр╕Бр╕гр╕Ш", "р╣Ар╕кр╕╡р╕вр╣Гр╕И", "р╕Ьр╕┤р╕Фр╕лр╕зр╕▒р╕З", "р╕гр╕│р╕Др╕▓р╕Н", "р╣Ар╕Бр╕ер╕╡р╕вр╕Ф", "р╕Бр╕ер╕▒р╕з", "р╕нр╕╢р╕Фр╕нр╕▒р╕Ф", "р╕Хр╕Бр╣Гр╕И"],
    "Neutral": ["р╣Ар╕Йр╕в р╣Ж", "р╣Др╕бр╣Ир╕гр╕╣р╣Йр╕кр╕╢р╕Бр╕нр╕░р╣Др╕г", "р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Вр╣Ир╕▓р╕зр╕кр╕▓р╕г"],
    "Others": ["р╕Ыр╕гр╕░р╕Кр╕Ф", "р╕Вр╕│р╕Вр╕▒р╕Щ", "р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡", "р╕кр╕▒р╕Ър╕кр╕Щ"]
}

# Reverse mapping for quick lookup
LABEL_TO_GROUP = {}
for group, labels in EMOTION_GROUPS.items():
    for label in labels:
        LABEL_TO_GROUP[label] = group

# === EMOTION PATTERNS ===
EMOTION_PATTERNS = {
    # === POSITIVE EMOTIONS ===
    "р╕Фр╕╡р╣Гр╕И": {
        "keywords": ["р╕Фр╕╡р╣Гр╕И", "р╕бр╕╡р╕Др╕зр╕▓р╕бр╕кр╕╕р╕В", "р╣Бр╕ор╕Ыр╕Ыр╕╡р╣Й", "р╕Ыр╕ер╕╖р╣Йр╕б", "р╕вр╕┤р╕Щр╕Фр╕╡", "р╣Ар╕ор╕З", "р╣Ар╕вр╣Й", "р╣Вр╕вр╣И", "р╣Ар╕Ир╣Лр╕З", "р╕Фр╕╡р╣Ир╣Гр╕И"],
        "patterns": [r"р╕Фр╕╡\s*р╣Гр╕И", r"р╕Ыр╕ер╕╖р╣Йр╕б", r"р╣Ар╕ор╕З\s*р╕Лр╕░", r"р╣Бр╕ор╕Ыр╕Ыр╕╡р╣Й", r"р╣Ар╕вр╣Й.*", r"р╣Вр╕вр╣И.*"],
        "emojis": ["ЁЯШК", "ЁЯШД", "ЁЯдЧ", "ЁЯШН", "ЁЯе░", "ЁЯШШ", "ЁЯШЖ", "ЁЯдй"],
        "score_range": (0.6, 1.0)
    },
    
    "р╕Кр╕нр╕Ъ": {
        "keywords": ["р╕Кр╕нр╕Ъ", "р╕гр╕▒р╕Б", "р╕Цр╕╣р╕Бр╣Гр╕И", "р╣Вр╕Ыр╕гр╕Ф", "р╕Ыр╕ер╕╖р╣Йр╕б", "р╕кр╕Щр╣Гр╕И", "р╕нр╕┤р╕Щ", "р╣Ар╕Др╕ер╕┤р╣Йр╕б"],
        "patterns": [r"р╕Кр╕нр╕Ъ.*р╕бр╕▓р╕Б", r"р╕гр╕▒р╕Б.*р╣Ар╕ер╕в", r"р╕Цр╕╣р╕Бр╣Гр╕И", r"р╣Вр╕Ыр╕гр╕Ф.*", r"р╕кр╕Щр╣Гр╕И.*р╕бр╕▓р╕Б"],
        "emojis": ["тЭдя╕П", "ЁЯТХ", "ЁЯШН", "ЁЯе░", "ЁЯШШ", "ЁЯТЦ", "ЁЯТЭ"],
        "score_range": (0.5, 0.9)
    },
    
    "р╕Лр╕╢р╣Йр╕Зр╣Гр╕И": {
        "keywords": ["р╕Лр╕╢р╣Йр╕З", "р╕Лр╕╢р╣Йр╕Зр╣Гр╕И", "р╕Щр╣Йр╕│р╕Хр╕▓р╕Лр╕╢р╕б", "р╕Ыр╕гр╕░р╕Чр╕▒р╕Ър╣Гр╕И", "р╕Лр╕▓р╕Ър╕Лр╕╢р╣Йр╕З", "р╕Хр╕╖р╣Йр╕Щр╕Хр╕▒р╕Щ", "р╕Лр╕╖р╣Ир╕Щр╣Гр╕к"],
        "patterns": [r"р╕Лр╕╢р╣Йр╕З.*р╣Гр╕И", r"р╕Ыр╕гр╕░р╕Чр╕▒р╕Ър╣Гр╕И", r"р╕Щр╣Йр╕│р╕Хр╕▓.*р╕Лр╕╢р╕б", r"р╕Лр╕▓р╕Ър╕Лр╕╢р╣Йр╕З"],
        "emojis": ["ЁЯШн", "ЁЯе║", "ЁЯШв", "ЁЯдз", "ЁЯТЮ"],
        "score_range": (0.4, 0.8)
    },
    
    "р╕Юр╕нр╣Гр╕И": {
        "keywords": ["р╕Юр╕нр╣Гр╕И", "р╣Вр╕нр╣Ар╕Д", "р╣Гр╕Кр╣Йр╣Др╕Фр╣Й", "р╕Ыр╕Бр╕Хр╕┤р╕Фр╕╡", "р╣Др╕бр╣Ир╣Ар╕Ыр╣Зр╕Щр╣Др╕г", "р╕Зр╕▓р╕б", "р╣Ар╕гр╕╡р╕вр╕Ър╕гр╣Йр╕нр╕в"],
        "patterns": [r"р╕Юр╕нр╣Гр╕И", r"р╣Вр╕нр╣Ар╕Д.*", r"р╣Гр╕Кр╣Йр╣Др╕Фр╣Й", r"р╕Ыр╕Бр╕Хр╕┤р╕Фр╕╡", r"р╣Ар╕гр╕╡р╕вр╕Ър╕гр╣Йр╕нр╕в"],
        "emojis": ["ЁЯСН", "ЁЯСМ", "ЁЯШМ", "ЁЯЩВ"],
        "score_range": (0.2, 0.6)
    },
    
    "р╕гр╕▒р╕Б": {
        "keywords": ["р╕гр╕▒р╕Б", "р╕лр╕ер╕Зр╕гр╕▒р╕Б", "р╣Бр╕Юр╕З", "р╣Ар╕ер╕┤р╕Я", "love", "р╕ор╕▒р╕Бр╣Ж", "р╕ор╕▒р╕Б", "р╕гр╕▒р╕Бр╕бр╕▓р╕Б"],
        "patterns": [r"р╕гр╕▒р╕Б.*р╕бр╕▓р╕Б", r"р╕лр╕ер╕Зр╕гр╕▒р╕Б", r"р╣Ар╕ер╕┤р╕Я.*", r"love.*", r"р╕ор╕▒р╕Б.*"],
        "emojis": ["тЭдя╕П", "ЁЯТХ", "ЁЯТЦ", "ЁЯТЭ", "ЁЯШН", "ЁЯе░", "ЁЯШШ"],
        "score_range": (0.7, 1.0)
    },
    
    # === NEGATIVE EMOTIONS ===
    "р╣Вр╕Бр╕гр╕Ш": {
        "keywords": ["р╣Вр╕Бр╕гр╕Ш", "р╕Йр╕╕р╕Щ", "р╣Вр╕бр╣Вр╕л", "р╣Бр╕Др╣Йр╕Щ", "р╕Вр╕╕р╣Ир╕Щр╕Вр╣Йр╕нр╕З", "р╣Ар╕Фр╕╖р╕нр╕Ф", "р╕Ър╣Йр╕▓", "р╕лр╣Ир╕зр╕вр╣Бр╕Хр╕Б", "р╣Бр╕вр╣И", "р╕Зр╕╡р╣Ир╣Ар╕Зр╣Ир╕▓"],
        "patterns": [r"р╣Вр╕Бр╕гр╕Ш.*р╕бр╕▓р╕Б", r"р╕Йр╕╕р╕Щ.*р╕Вр╕▓р╕Ф", r"р╣Вр╕бр╣Вр╕л", r"р╣Бр╕Др╣Йр╕Щ.*", r"р╕лр╣Ир╕зр╕в.*р╣Бр╕Хр╕Б", r"р╕Ър╣Йр╕▓.*", r"р╣Бр╕вр╣И.*р╕бр╕▓р╕Б"],
        "emojis": ["ЁЯШа", "ЁЯШб", "ЁЯдм", "ЁЯС┐", "ЁЯТв", "ЁЯШд"],
        "score_range": (-1.0, -0.6)
    },
    
    "р╣Ар╕кр╕╡р╕вр╣Гр╕И": {
        "keywords": ["р╣Ар╕кр╕╡р╕вр╣Гр╕И", "р╣Ар╕ир╕гр╣Йр╕▓", "р╣Гр╕Ир╕лр╕▓р╕в", "р╕Ыр╕зр╕Фр╣Гр╕И", "р╣Ар╕ир╕гр╣Йр╕▓р╣Вр╕ир╕Б", "р╣Вр╕ир╕Бр╣Ар╕ир╕гр╣Йр╕▓", "р╣Ар╕кр╕╡р╕вр╕Фр╕▓р╕в", "р╕Щр╣Ир╕▓р╣Ар╕кр╕╡р╕вр╣Гр╕И"],
        "patterns": [r"р╣Ар╕кр╕╡р╕вр╣Гр╕И", r"р╣Ар╕ир╕гр╣Йр╕▓.*р╕бр╕▓р╕Б", r"р╣Гр╕Ир╕лр╕▓р╕в", r"р╕Ыр╕зр╕Фр╣Гр╕И", r"р╣Ар╕ир╕гр╣Йр╕▓р╣Вр╕ир╕Б"],
        "emojis": ["ЁЯШв", "ЁЯШн", "ЁЯШЮ", "тШ╣я╕П", "ЁЯШФ", "ЁЯТФ"],
        "score_range": (-0.8, -0.4)
    },
    
    "р╕Ьр╕┤р╕Фр╕лр╕зр╕▒р╕З": {
        "keywords": ["р╕Ьр╕┤р╕Фр╕лр╕зр╕▒р╕З", "р╕лр╕зр╕▒р╕Зр╣Ар╕Бр╕┤р╕Щ", "р╕Др╕▓р╕Фр╕лр╕зр╕▒р╕З", "р╕Чр╣Йр╕н", "р╕лр╕бр╕Фр╕лр╕зр╕▒р╕З", "р╣Др╕бр╣Ир╣Др╕Фр╣Йр╕Фр╕▒р╕Зр╣Гр╕И"],
        "patterns": [r"р╕Ьр╕┤р╕Фр╕лр╕зр╕▒р╕З", r"р╕лр╕зр╕▒р╕З.*р╣Ар╕Бр╕┤р╕Щ", r"р╕Др╕▓р╕Фр╕лр╕зр╕▒р╕З.*р╕бр╕▓р╕Б", r"р╕Чр╣Йр╕н.*", r"р╕лр╕бр╕Фр╕лр╕зр╕▒р╕З"],
        "emojis": ["ЁЯШЮ", "ЁЯШФ", "ЁЯШУ", "ЁЯШй", "ЁЯШд"],
        "score_range": (-0.7, -0.3)
    },
    
    "р╕гр╕│р╕Др╕▓р╕Н": {
        "keywords": ["р╕гр╕│р╕Др╕▓р╕Н", "р╕Щр╣Ир╕▓р╕гр╕│р╕Др╕▓р╕Н", "р╣Ар╕Ър╕╖р╣Ир╕н", "р╕лр╕Щр╣Ир╕▓р╕в", "р╣Ар╕Лр╣Зр╕З", "р╕Зр╣Ир╕зр╕З", "р╣Ар╕Др╕гр╕╡р╕вр╕Ф", "р╣Ар╕лр╕Щр╕╖р╣Ир╕нр╕в"],
        "patterns": [r"р╕гр╕│р╕Др╕▓р╕Н", r"р╣Ар╕Ър╕╖р╣Ир╕н.*р╕бр╕▓р╕Б", r"р╕лр╕Щр╣Ир╕▓р╕в.*", r"р╣Ар╕Лр╣Зр╕З.*", r"р╣Ар╕Др╕гр╕╡р╕вр╕Ф.*"],
        "emojis": ["ЁЯШТ", "ЁЯЩД", "ЁЯШд", "ЁЯШС", "ЁЯШл", "ЁЯШй"],
        "score_range": (-0.6, -0.2)
    },
    
    "р╣Ар╕Бр╕ер╕╡р╕вр╕Ф": {
        "keywords": ["р╣Ар╕Бр╕ер╕╡р╕вр╕Ф", "р╕Вр╕вр╕░р╣Бр╕Вр╕вр╕З", "р╣Бр╕Др╣Йр╕Щ", "р╣Бр╕Бр╕ер╣Йр╕З", "р╣Др╕бр╣Ир╕Кр╕нр╕Ъ", "р╕Хр╣Ир╕нр╕Хр╣Йр╕▓р╕Щ"],
        "patterns": [r"р╣Ар╕Бр╕ер╕╡р╕вр╕Ф.*р╕бр╕▓р╕Б", r"р╕Вр╕вр╕░р╣Бр╕Вр╕вр╕З", r"р╣Бр╕Др╣Йр╕Щ.*", r"р╣Др╕бр╣Ир╕Кр╕нр╕Ъ.*р╣Ар╕ер╕в"],
        "emojis": ["ЁЯШб", "ЁЯдм", "ЁЯС┐", "ЁЯШа", "ЁЯТв"],
        "score_range": (-1.0, -0.7)
    },
    
    "р╕Бр╕ер╕▒р╕з": {
        "keywords": ["р╕Бр╕ер╕▒р╕з", "р╕лр╕зр╕▓р╕Фр╕Бр╕ер╕▒р╕з", "р╕Хр╕Бр╣Гр╕И", "р╕зр╕┤р╕Хр╕Б", "р╕Бр╕▒р╕Зр╕зр╕е", "р╣Ар╕Др╕гр╕╡р╕вр╕Ф", "р╕лр╕зр╕▒р╣Ир╕Щ", "р╕Хр╕╖р╣Ир╕Щр╕Бр╕ер╕▒р╕з"],
        "patterns": [r"р╕Бр╕ер╕▒р╕з.*р╕бр╕▓р╕Б", r"р╕лр╕зр╕▓р╕Фр╕Бр╕ер╕▒р╕з", r"р╕Хр╕Бр╣Гр╕И.*", r"р╕зр╕┤р╕Хр╕Б.*", r"р╕Бр╕▒р╕Зр╕зр╕е.*"],
        "emojis": ["ЁЯШи", "ЁЯШ░", "ЁЯШ▒", "ЁЯШз", "ЁЯлг", "ЁЯШ│"],
        "score_range": (-0.8, -0.3)
    },
    
    "р╕нр╕╢р╕Фр╕нр╕▒р╕Ф": {
        "keywords": ["р╕нр╕╢р╕Фр╕нр╕▒р╕Ф", "р╕нр╕▒р╕Ър╕нр╕▓р╕в", "р╣Ар╕Бр╣Йр╕н", "р╣Др╕бр╣Ир╕кр╕Ър╕▓р╕вр╣Гр╕И", "р╕Бр╕Фр╕Фр╕▒р╕Щ", "р╕Вр╕▒р╕Фр╣Гр╕И"],
        "patterns": [r"р╕нр╕╢р╕Фр╕нр╕▒р╕Ф", r"р╕нр╕▒р╕Ър╕нр╕▓р╕в", r"р╣Др╕бр╣Ир╕кр╕Ър╕▓р╕вр╣Гр╕И", r"р╕Бр╕Фр╕Фр╕▒р╕Щ", r"р╕Вр╕▒р╕Фр╣Гр╕И"],
        "emojis": ["ЁЯШг", "ЁЯШЦ", "ЁЯШл", "ЁЯШд", "ЁЯШ░"],
        "score_range": (-0.6, -0.2)
    },
    
    "р╕Хр╕Бр╣Гр╕И": {
        "keywords": ["р╕Хр╕Бр╣Гр╕И", "р╕кр╕░р╕Фр╕╕р╣Йр╕З", "р╣Вр╕лр╕вр╕З", "р╕Хр╕Бр╕Хр╕░р╕ер╕╢р╕З", "р╕Хр╕░р╕ер╕╢р╕З", "р╕лр╕зр╕▓р╕Фр╣Ар╕кр╕╡р╕вр╕з"],
        "patterns": [r"р╕Хр╕Бр╣Гр╕И.*р╕бр╕▓р╕Б", r"р╕кр╕░р╕Фр╕╕р╣Йр╕З", r"р╣Вр╕лр╕вр╕З", r"р╕Хр╕Бр╕Хр╕░р╕ер╕╢р╕З", r"р╕Хр╕░р╕ер╕╢р╕З"],
        "emojis": ["ЁЯШ▒", "ЁЯШи", "ЁЯШ│", "ЁЯли", "ЁЯШз"],
        "score_range": (-0.5, 0.0)
    },
    
    # === NEUTRAL EMOTIONS ===
    "р╣Ар╕Йр╕в р╣Ж": {
        "keywords": ["р╣Ар╕Йр╕в", "р╕Шр╕гр╕гр╕бр╕Фр╕▓", "р╕Ыр╕Бр╕Хр╕┤", "р╣Вр╕нр╣Ар╕Д", "р╣Гр╕Кр╣Йр╣Др╕Фр╣Й", "р╕Юр╕нр╣Гр╕Кр╣Й", "р╣Др╕бр╣Ир╣Ар╕Ыр╣Зр╕Щр╣Др╕г"],
        "patterns": [r"р╣Ар╕Йр╕в.*р╣Ж", r"р╕Шр╕гр╕гр╕бр╕Фр╕▓", r"р╕Ыр╕Бр╕Хр╕┤.*", r"р╣Вр╕нр╣Ар╕Д", r"р╣Др╕бр╣Ир╣Ар╕Ыр╣Зр╕Щр╣Др╕г"],
        "emojis": ["ЁЯШР", "ЁЯЩВ", "ЁЯШ╢", "ЁЯШС"],
        "score_range": (-0.1, 0.1)
    },
    
    "р╣Др╕бр╣Ир╕гр╕╣р╣Йр╕кр╕╢р╕Бр╕нр╕░р╣Др╕г": {
        "keywords": ["р╣Др╕бр╣Ир╕гр╕╣р╣Йр╕кр╕╢р╕Б", "р╕Кр╕▓", "р╣Ар╕Йр╕в", "р╣Др╕бр╣Ир╕кр╕Щ", "р╣Др╕бр╣Ир╣Бр╕Др╕гр╣М", "р╣Др╕бр╣Ир╣Ар╕Вр╣Йр╕▓р╣Гр╕И"],
        "patterns": [r"р╣Др╕бр╣Ир╕гр╕╣р╣Йр╕кр╕╢р╕Б.*р╕нр╕░р╣Др╕г", r"р╕Кр╕▓.*", r"р╣Др╕бр╣Ир╕кр╕Щ.*", r"р╣Др╕бр╣Ир╣Бр╕Др╕гр╣М"],
        "emojis": ["ЁЯШ╢", "ЁЯШР", "ЁЯд╖тАНтЩАя╕П", "ЁЯд╖тАНтЩВя╕П"],
        "score_range": (-0.05, 0.05)
    },
    
    "р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Вр╣Ир╕▓р╕зр╕кр╕▓р╕г": {
        "keywords": ["р╕Вр╣Йр╕нр╕бр╕╣р╕е", "р╕Вр╣Ир╕▓р╕з", "р╕гр╕▓р╕вр╕Зр╕▓р╕Щ", "р╣Бр╕Ир╣Йр╕З", "р╕Ър╕нр╕Б", "р╕нр╕▒р╕Ыр╣Ар╕Фр╕Х", "р╕кр╕│р╕Др╕▒р╕Н"],
        "patterns": [r"р╕Вр╣Йр╕нр╕бр╕╣р╕е.*", r"р╕Вр╣Ир╕▓р╕з.*", r"р╕гр╕▓р╕вр╕Зр╕▓р╕Щ.*", r"р╣Бр╕Ир╣Йр╕З.*", r"р╕нр╕▒р╕Ыр╣Ар╕Фр╕Х.*"],
        "emojis": ["ЁЯУ░", "ЁЯУК", "ЁЯУИ", "ЁЯУв", "тД╣я╕П"],
        "score_range": (0.0, 0.0)
    },
    
    # === OTHERS (COMPLEX EMOTIONS) ===
    "р╕Ыр╕гр╕░р╕Кр╕Ф": {
        "keywords": ["р╕Ыр╕гр╕░р╕Кр╕Ф", "р╣Ар╕лр╕Щр╣Зр╕Ър╣Бр╕Щр╕б", "р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡", "р╣Бр╕Фр╕Бр╕Фр╕▒р╕Щ", "р╕Ир╕┤р╕Бр╕Бр╕▒р╕Ф", "р╕нр╕╡р╕Фр╕нр╕Б"],
        "patterns": [r"р╕Ыр╕гр╕░р╕Кр╕Ф.*", r"р╣Ар╕лр╕Щр╣Зр╕Ър╣Бр╕Щр╕б", r"р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡.*", r"р╣Бр╕Фр╕Бр╕Фр╕▒р╕Щ", r"р╕Ир╕┤р╕Бр╕Бр╕▒р╕Ф"],
        "emojis": ["ЁЯШП", "ЁЯЩД", "ЁЯШТ", "ЁЯШд"],
        "score_range": (-0.4, -0.1)
    },
    
    "р╕Вр╕│р╕Вр╕▒р╕Щ": {
        "keywords": ["р╕Вр╕│", "р╕Хр╕ер╕Б", "555", "р╕ор╕▓", "р╣Ар╕ор╕ор╕▓", "р╕кр╕Щр╕╕р╕Б", "р╣Вр╕ер╕Бр╣Бр╕Хр╕Б", "р╕Др╕гр╕╖р╣Ир╕Щр╣Ар╕Др╕гр╕З"],
        "patterns": [r"р╕Вр╕│.*", r"р╕Хр╕ер╕Б.*", r"555+", r"р╕ор╕▓+", r"р╣Ар╕ор╕ор╕▓", r"р╕кр╕Щр╕╕р╕Б.*"],
        "emojis": ["ЁЯШВ", "ЁЯдг", "ЁЯШЖ", "ЁЯШД", "ЁЯШБ", "ЁЯдк", "ЁЯШЬ"],
        "score_range": (0.3, 0.8)
    },
    
    "р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡": {
        "keywords": ["р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡", "р╕Ыр╕гр╕░р╕Кр╕Ф", "р╣Ар╕лр╕Щр╣Зр╕Ъ", "р╣Бр╕Щр╕б", "р╕Ир╕┤р╕Бр╕Бр╕▒р╕Ф", "р╣Бр╕Бр╕ер╣Йр╕З"],
        "patterns": [r"р╣Ар╕кр╕╡р╕вр╕Фр╕кр╕╡.*", r"р╕Ыр╕гр╕░р╕Кр╕Ф.*", r"р╣Ар╕лр╕Щр╣Зр╕Ъ.*р╣Бр╕Щр╕б", r"р╕Ир╕┤р╕Бр╕Бр╕▒р╕Ф.*"],
        "emojis": ["ЁЯШП", "ЁЯЩД", "ЁЯШТ"],
        "score_range": (-0.5, -0.2)
    },
    
    "р╕кр╕▒р╕Ър╕кр╕Щ": {
        "keywords": ["р╕кр╕▒р╕Ър╕кр╕Щ", "р╕Зр╕З", "р╣Ар╕Вр╣Йр╕▓р╣Гр╕Ир╣Др╕бр╣Ир╣Др╕Фр╣Й", "р╣Бр╕Ыр╕ер╕Б", "р╕Йр╕Зр╕Щ", "р╕Йр╕Зр╕Щр╕кр╕Щр╣Ар╕Чр╣Ир╕лр╣М"],
        "patterns": [r"р╕кр╕▒р╕Ър╕кр╕Щ", r"р╕Зр╕З.*", r"р╣Ар╕Вр╣Йр╕▓р╣Гр╕Ир╣Др╕бр╣Ир╣Др╕Фр╣Й", r"р╣Бр╕Ыр╕ер╕Б.*", r"р╕Йр╕Зр╕Щ.*"],
        "emojis": ["ЁЯШХ", "ЁЯдФ", "ЁЯШ╡тАНЁЯТл", "ЁЯлд", "ЁЯШ╡"],
        "score_range": (-0.2, 0.2)
    }
}

# === COMPREHENSIVE CONTEXT PATTERNS ===
CONTEXT_PATTERNS = {
    # === р╕гр╕░р╕Фр╕▒р╕Ър╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╕Чр╕▓р╕Зр╕Бр╕▓р╕г ===
    "formal": ["р╕Др╕гр╕▒р╕Ъ", "р╕Др╣Ир╕░", "р╕Др╕░", "р╕Вр╕н", "р╕Бр╕гр╕╕р╕Ур╕▓", "р╕кр╕зр╕▒р╕кр╕Фр╕╡", "р╕Вр╕нр╕Ър╕Др╕╕р╕У", "р╕Чр╣Ир╕▓р╕Щ", "р╕Др╕╕р╕У", "р╕Юр╕╡р╣И", "р╕Щр╣Йр╕нр╕З", "р╣Ар╕гр╕╡р╕вр╕Щ", "р╕Фр╣Йр╕зр╕вр╕Др╕зр╕▓р╕бр╣Ар╕Др╕▓р╕гр╕Ю"],
    "informal": ["р╕Щр╕░", "р╣Ар╕Щр╕нр╕░", "р╕нр╕░", "р╣Ар╕нр╣Йр╕в", "р╣Ар╕нр╕н", "р╕Вр╕нр╕З", "555", "р╕ор╕▓", "р╕Ир╣Йр╕░", "р╕Ир╣Лр╕▓", "р╕зр╣Ир╕░", "р╕зр╕░", "р╣Ар╕ор╣Йр╕в"],
    "slang": ["р╣Вр╕Др╕Хр╕г", "р╣Ар╕Яр╕╡р╣Йр╕вр╕з", "р╣Ар╕Чр╕Ю", "р╣Бр╕бр╣Ир╕З", "р╕Др╕зр╕в", "р╕Ър╕┤р╕Щ", "р╣Ар╕Яр╕╡р╣Йр╕вр╕б", "р╕Кр╕┤р╕Ъ", "р╣Ар╕Фр╣Зр╕Ф", "р╕Ыр╕▒р╕З", "р╕лр╣Ир╕зр╕в", "р╕Лр╕зр╕в"],
    
    # === р╕Др╕зр╕▓р╕бр╕кр╕▒р╕бр╕Юр╕▒р╕Щр╕Шр╣Мр╕кр╣Ир╕зр╕Щр╕Хр╕▒р╕з ===
    "personal": ["р╕Бр╕╣", "р╕бр╕╢р╕З", "р╣Ар╕гр╕▓", "р╕Йр╕▒р╕Щ", "р╕Др╕┤р╕Ф", "р╕гр╕╣р╣Йр╕кр╕╢р╕Б", "р╣Гр╕И", "р╕лр╕▒р╕зр╣Гр╕И", "р╕Хр╕▒р╕зр╣Ар╕нр╕З", "р╕кр╣Ир╕зр╕Щр╕Хр╕▒р╕з"],
    "intimate": ["р╕Чр╕╡р╣Ир╕гр╕▒р╕Б", "р╕лр╕зр╕▓р╕Щр╣Гр╕И", "р╕Фр╕▓р╕гр╣Мр╕ер╕┤р╣Ир╕З", "р╕ор╕▒р╕Щр╕Щр╕╡р╣И", "р╣Ар╕Ър╕Ър╕╡р╣Й", "р╕Др╕Щр╕Фр╕╡", "р╣Ар╕кр╕╖р╕н", "р╕лр╕Щр╕╣р╣Ар╕нр╕З"],
    "friendly": ["р╣Ар╕Юр╕╖р╣Ир╕нр╕Щ", "р╣Ар╕Яр╕гр╣Йр╕Щ", "р╕Юр╕зр╕Бр╣Ар╕гр╕▓", "р╣Бр╕Бр╣Кр╕З", "р╕Бр╕ер╕╕р╣Ир╕б", "р╕Др╕Щр╣Ар╕Бр╣Ир╕▓", "р╕Юр╕╡р╣Ир╕Щр╣Йр╕нр╕З"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕кр╕╖р╣Ир╕нр╕кр╕▒р╕Зр╕Др╕б ===
    "social_media": ["р╣Бр╕Кр╕гр╣М", "р╣Др╕ер╕Др╣М", "р╕Др╕нр╕бр╣Ар╕бр╕Щр╕Хр╣М", "р╣Вр╕Юр╕кр╕Хр╣М", "р╣Бр╕Чр╣Зр╕Б", "р╣Ар╕Яр╕кр╕Ър╕╕р╣Кр╕Д", "р╣Др╕нр╕Ир╕╡", "р╕Чр╕зр╕┤р╕Хр╣Ар╕Хр╕нр╕гр╣М", "р╕Хр╕┤р╣Кр╕Бр╕Хр╣Кр╕нр╕Б", "р╕вр╕╣р╕Чр╕╣р╕Ы"],
    "news_media": ["р╕Вр╣Ир╕▓р╕з", "р╕гр╕▓р╕вр╕Зр╕▓р╕Щ", "р╣Бр╕Ир╣Йр╕Зр╕Вр╣Ир╕▓р╕з", "р╕Вр╣Йр╕нр╕бр╕╣р╕е", "р╕нр╕▒р╕Ыр╣Ар╕Фр╕Х", "р╕Ыр╕гр╕░р╕Бр╕▓р╕и", "р╣Бр╕Цр╕ер╕Зр╕Бр╕▓р╕гр╕Ур╣М", "р╕кр╕│р╕Др╕▒р╕Н", "р╕Фр╣Ир╕зр╕Щ"],
    "review": ["р╕гр╕╡р╕зр╕┤р╕з", "р╕Чр╕Фр╕ер╕нр╕З", "р╣Гр╕Кр╣Йр╕Фр╕╣", "р╕ер╕нр╕З", "р╕Ыр╕гр╕░р╕кр╕Ър╕Бр╕▓р╕гр╕Ур╣М", "р╕Др╕╕р╕Ур╕ар╕▓р╕Ю", "р╕Ър╕гр╕┤р╕Бр╕▓р╕г", "р╕кр╕┤р╕Щр╕Др╣Йр╕▓", "р╕гр╣Йр╕▓р╕Щ"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕нр╕▓р╕гр╕бр╕Ур╣М ===
    "complaint": ["р╕Ър╣Ир╕Щ", "р╕гр╣Йр╕нр╕Зр╣Ар╕гр╕╡р╕вр╕Щ", "р╣Бр╕Ир╣Йр╕Зр╕Ыр╕▒р╕Нр╕лр╕▓", "р╣Др╕бр╣Ир╣Др╕Фр╣Й", "р╣Ар╕кр╕╡р╕в", "р╕лр╣Ир╕зр╕в", "р╣Бр╕вр╣И", "р╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф", "р╕Кр╣Йр╕▓"],
    "praise": ["р╕Кр╕б", "р╕вр╕Бр╕вр╣Ир╕нр╕З", "р╕Фр╕╡", "р╣Ар╕вр╕╡р╣Ир╕вр╕б", "р╕Ыр╕гр╕░р╕Чр╕▒р╕Ър╣Гр╕И", "р╕Кр╕нр╕Ъ", "р╕гр╕▒р╕Б", "р╕кр╕╕р╕Фр╕вр╕нр╕Ф", "р╣Ар╕Ир╣Лр╕З", "р╣Ар╕Чр╕Ю"],
    "question": ["р╕Др╕гр╕▒р╕Ъ", "р╕Др╕░", "р╕бр╕▒р╣Йр╕в", "р╕лр╕гр╕╖р╕н", "р╣Др╕лр╕б", "р╕нр╕░р╣Др╕г", "р╕Чр╕│р╣Др╕б", "р╕вр╕▒р╕Зр╣Др╕З", "р╣Ар╕бр╕╖р╣Ир╕нр╣Др╕лр╕гр╣И", "р╕Чр╕╡р╣Ир╣Др╕лр╕Щ"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕кр╕Цр╕▓р╕Щр╕Бр╕▓р╕гр╕Ур╣М ===
    "emergency": ["р╕Фр╣Ир╕зр╕Щ", "р╣Ар╕гр╣Ир╕Зр╕Фр╣Ир╕зр╕Щ", "р╕Йр╕╕р╕Бр╣Ар╕Йр╕┤р╕Щ", "р╕Кр╣Ир╕зр╕в", "р╕Ыр╕▒р╕Нр╕лр╕▓", "р╣Ар╕кр╕╡р╕в", "р╕Юр╕▒р╕З", "р╕нр╕▒р╕Щр╕Хр╕гр╕▓р╕в", "р╕зр╕┤р╕Бр╕др╕Х"],
    "celebration": ["р╕вр╕┤р╕Щр╕Фр╕╡", "р╣Бр╕кр╕Фр╕Зр╕Др╕зр╕▓р╕бр╕вр╕┤р╕Щр╕Фр╕╡", "р╕Вр╕нр╣Бр╕кр╕Фр╕Зр╕Др╕зр╕▓р╕бр╕вр╕┤р╕Щр╕Фр╕╡", "р╕Фр╕╡р╣Гр╕И", "р╕Ыр╕ер╕╖р╣Йр╕бр╕Ыр╕╡р╕Хр╕┤", "р╣Ар╕ор╕З", "р╣Вр╕Кр╕Др╕Фр╕╡"],
    "condolence": ["р╣Ар╕кр╕╡р╕вр╣Гр╕И", "р╣Бр╕кр╕Фр╕Зр╕Др╕зр╕▓р╕бр╣Ар╕кр╕╡р╕вр╣Гр╕И", "р╕Вр╕нр╣Бр╕кр╕Фр╕Зр╕Др╕зр╕▓р╕бр╣Ар╕кр╕╡р╕вр╣Гр╕И", "р╣Ар╕ир╕гр╣Йр╕▓", "р╕нр╕▓р╕ер╕▒р╕в", "р╕Др╕┤р╕Фр╕Цр╕╢р╕З"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕зр╕▒р╕Тр╕Щр╕Шр╕гр╕гр╕б ===
    "religious": ["р╕Ър╕╕р╕Н", "р╕Бр╕гр╕гр╕б", "р╕Шр╕гр╕гр╕б", "р╕Юр╕гр╕░", "р╕зр╕▒р╕Ф", "р╕Щр╕бр╕▒р╕кр╕Бр╕▓р╕г", "р╣Др╕лр╕зр╣Й", "р╕ир╕▓р╕кр╕Щр╕▓", "р╕Ър╕▓р╕Ы", "р╕Бр╕╕р╕ир╕е"],
    "traditional": ["р╕Ыр╕гр╕░р╣Ар╕Юр╕Ур╕╡", "р╕зр╕▒р╕Тр╕Щр╕Шр╕гр╕гр╕б", "р╣Др╕Чр╕в", "р╣Вр╕Ър╕гр╕▓р╕У", "р╕Фр╕▒р╣Йр╕Зр╣Ар╕Фр╕┤р╕б", "р╕ар╕╣р╕бр╕┤р╕Ыр╕▒р╕Нр╕Нр╕▓", "р╕Кр╕▓р╕зр╕Ър╣Йр╕▓р╕Щ"],
    "modern": ["р╕Чр╕▒р╕Щр╕кр╕бр╕▒р╕в", "р╣Вр╕бр╣Ар╕Фр╕┤р╕гр╣Мр╕Щ", "р╣Др╕ор╣Ар╕Чр╕Д", "р╕Фр╕┤р╕Ир╕┤р╕Чр╕▒р╕е", "р╕нр╕нр╕Щр╣Др╕ер╕Щр╣М", "р╣Бр╕нр╕Ы", "р╣Др╕нр╕Чр╕╡", "р╣Ар╕Чр╕Др╣Вр╕Щр╣Вр╕ер╕вр╕╡"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕ар╕╣р╕бр╕┤р╕ир╕▓р╕кр╕Хр╕гр╣М ===
    "central": ["р╕Бр╕гр╕╕р╕Зр╣Ар╕Чр╕Ю", "р╕Бр╕Чр╕б", "р╣Ар╕бр╕╖р╕нр╕Зр╕лр╕ер╕зр╕З", "р╕ар╕▓р╕Др╕Бр╕ер╕▓р╕З", "р╕Ир╕▒р╕Зр╕лр╕зр╕▒р╕Фр╣Гр╕Бр╕ер╣Йр╣Ар╕Др╕╡р╕вр╕З"],
    "northern": ["р╣Ар╕Кр╕╡р╕вр╕Зр╣Гр╕лр╕бр╣И", "р╕ар╕▓р╕Др╣Ар╕лр╕Щр╕╖р╕н", "р╕ер╣Йр╕▓р╕Щр╕Щр╕▓", "р╕Др╕│р╣Ар╕бр╕╖р╕нр╕З", "р╕Щр╕▓", "р╕Ыр╣Ир╕▓"],
    "southern": ["р╣Гр╕Хр╣Й", "р╕ар╕▓р╕Др╣Гр╕Хр╣Й", "р╕Чр╕░р╣Ар╕е", "р╕Ыр╕ер╕▓", "р╕вр╕▓р╕Зр╕Юр╕▓р╕гр╕▓", "р╕Ыр╕▓р╕ер╣Мр╕б"],
    "northeastern": ["р╕нр╕╡р╕кр╕▓р╕Щ", "р╕ар╕▓р╕Др╕нр╕╡р╕кр╕▓р╕Щ", "р╕кр╣Йр╕бр╕Хр╕│", "р╕ер╕▓р╕з", "р╕Вр╣Йр╕▓р╕зр╣Ар╕лр╕Щр╕╡р╕вр╕з", "р╣Бр╕Ир╣Ир╕з"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕нр╕▓р╕вр╕╕/р╕гр╕╕р╣Ир╕Щ ===
    "gen_z": ["р╕Ыр╕▒р╕З", "р╣Ар╕Фр╣Зр╕Ф", "р╕Яр╕┤р╕Щ", "р╕Кр╕┤р╕е", "р╣Ар╕Яр╕ер╣Зр╕Бр╕Лр╣М", "р╣Вр╕Ъ", "р╕ер╕┤р╕Х", "р╣Др╕зр╕Ър╣М", "р╕Др╕нр╕Щр╣Ар╕Чр╕Щр╕Хр╣М"],
    "millennial": ["р╣Вр╕нр╣Ар╕Д", "р╣Ар╕Яр╕к", "р╣Др╕ер╕Щр╣М", "р╕нр╕┤р╕Щр╕кр╕Хр╕▓", "р╕Лр╕╡р╕гр╕╡р╣Ир╕вр╣М", "р╕вр╕╣р╕Чр╕╣р╕Ы", "р╕Бр╕╣р╣Ар╕Бр╕┤р╕е"],
    "gen_x": ["р╕Ир╕гр╕┤р╕Зр╕лр╕гр╕╖р╕нр╣Ар╕Ыр╕ер╣Ир╕▓", "р╣Др╕бр╣Ир╣Ар╕Кр╕╖р╣Ир╕н", "р╕кр╕бр╕▒р╕вр╕Бр╣Ир╕нр╕Щ", "р╕Хр╕нр╕Щр╕лр╕Щр╕╕р╣Ир╕б", "р╕Ыр╕▒р╕Ир╕Ир╕╕р╕Ър╕▒р╕Щ"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕зр╕┤р╕Кр╕▓р╕Кр╕╡р╕Ю ===
    "business": ["р╕Шр╕╕р╕гр╕Бр╕┤р╕И", "р╕Бр╕▓р╕гр╕Хр╕ер╕▓р╕Ф", "р╕Вр╕▓р╕в", "р╕ер╕╣р╕Бр╕Др╣Йр╕▓", "р╕Бр╕│р╣Др╕г", "р╕Вр╕▓р╕Фр╕Чр╕╕р╕Щ", "р╕ер╕Зр╕Чр╕╕р╕Щ"],
    "education": ["р╣Ар╕гр╕╡р╕вр╕Щ", "р╕кр╕нр╕Щ", "р╕Др╕гр╕╣", "р╕Щр╕▒р╕Бр╣Ар╕гр╕╡р╕вр╕Щ", "р╕Щр╕▒р╕Бр╕ир╕╢р╕Бр╕йр╕▓", "р╕Бр╕▓р╕гр╕ир╕╢р╕Бр╕йр╕▓", "р╕зр╕┤р╕Кр╕▓"],
    "healthcare": ["р╕лр╕бр╕н", "р╕Др╕ер╕┤р╕Щр╕┤р╕Б", "р╣Вр╕гр╕Зр╕Юр╕вр╕▓р╕Ър╕▓р╕е", "р╕вр╕▓", "р╕гр╕▒р╕Бр╕йр╕▓", "р╕кр╕╕р╕Вр╕ар╕▓р╕Ю", "р╕Ыр╣Ир╕зр╕в"],
    "government": ["р╕гр╕▓р╕Кр╕Бр╕▓р╕г", "р╕гр╕▒р╕Рр╕Ър╕▓р╕е", "р╕Щр╣Вр╕вр╕Ър╕▓р╕в", "р╕Бр╕Ор╕лр╕бр╕▓р╕в", "р╕гр╕░р╣Ар╕Ър╕╡р╕вр╕Ъ", "р╕Вр╣Йр╕▓р╕гр╕▓р╕Кр╕Бр╕▓р╕г"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╣Ар╕Чр╕Др╣Вр╕Щр╣Вр╕ер╕вр╕╡ ===
    "gaming": ["р╣Ар╕Бр╕б", "р╣Ар╕ер╣Ир╕Щ", "р╣Бр╕гр╕Зр╕Др╣М", "р╕кр╕Бр╕┤р╕е", "р╕нр╕▒р╕Ю", "р╣Ар╕ер╣Ар╕зр╕е", "PvP", "р╣Др╕нр╣Ар╕Чр╕б"],
    "tech": ["р╣Вр╕Др╣Йр╕Ф", "р╣Вр╕Ыр╕гр╣Бр╕Бр╕гр╕б", "р╣Бр╕нр╕Ы", "р╣Ар╕зр╣Зр╕Ъ", "AI", "р╕бр╕╖р╕нр╕Цр╕╖р╕н", "р╕Др╕нр╕б", "р╕Лр╕нр╕Яр╕Хр╣Мр╣Бр╕зр╕гр╣М"],
    "crypto": ["р╕Др╕гр╕┤р╕Ыр╣Вр╕Х", "р╕Ър╕┤р╕Чр╕Др╕нр╕вр╕Щр╣М", "р╣Ар╕лр╕гр╕╡р╕вр╕Н", "р╣Ар╕Чр╕гр╕Ф", "р╕Вр╕╕р╕Ф", "р╕зр╕нр╕ер╣Ар╕ер╣Зр╕Х", "NFT"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕Др╕зр╕▓р╕бр╕Ър╕▒р╕Щр╣Ар╕Чр╕┤р╕З ===
    "music": ["р╣Ар╕Юр╕ер╕З", "р╕ир╕┤р╕ер╕Ыр╕┤р╕Щ", "р╕Др╕нр╕Щр╣Ар╕кр╕┤р╕гр╣Мр╕Х", "р╕нр╕▒р╕ер╕Ър╕▒р╣Йр╕б", "р╣Ар╕Щр╕╖р╣Йр╕нр╣Ар╕Юр╕ер╕З", "р╕Фр╕Щр╕Хр╕гр╕╡", "р╣Бр╕гр╣Зр╕Ы"],
    "movie": ["р╕лр╕Щр╕▒р╕З", "р╕Лр╕╡р╕гр╕╡р╣Ир╕вр╣М", "р╕Щр╕▒р╕Бр╣Бр╕кр╕Фр╕З", "р╕Ьр╕╣р╣Йр╕Бр╕│р╕Бр╕▒р╕Ъ", "р╣Вр╕гр╕Зр╕лр╕Щр╕▒р╕З", "Netflix", "р╕Фр╕╣р╕лр╕Щр╕▒р╕З"],
    "sports": ["р╕Бр╕╡р╕мр╕▓", "р╕Чр╕╡р╕б", "р╣Бр╕Вр╣Ир╕З", "р╕Кр╕Щр╕░", "р╣Бр╕Юр╣Й", "р╕Яр╕╕р╕Хр╕Ър╕нр╕е", "р╕Ър╕▓р╕к", "р╕зр╕┤р╣Ир╕З"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕Др╕зр╕▓р╕бр╕гр╕▒р╕Б ===
    "romance": ["р╕Др╕╣р╣Ир╕гр╕▒р╕Б", "р╣Бр╕Яр╕Щ", "р╕лр╕зр╕▓р╕Щ", "р╣Вр╕гр╣Бр╕бр╕Щр╕Хр╕┤р╕Б", "р╕Ир╕╡р╕Ъ", "р╣Ар╕Фр╕Ч", "р╕Др╕Ъ", "р╕гр╕▒р╕Б"],
    "breakup": ["р╣Ар╕ер╕┤р╕Бр╕Бр╕▒р╕Щ", "р╕Чр╕┤р╣Йр╕З", "р╕лр╕ер╕нр╕Б", "р╕Щр╕нр╕Бр╣Гр╕И", "р╣Ар╕ир╕гр╣Йр╕▓р╣Гр╕И", "р╕Др╕┤р╕Фр╕Цр╕╢р╕З", "р╕лр╕бр╕Фр╕гр╕▒р╕Б"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕Бр╕▓р╕гр╣Ар╕Зр╕┤р╕Щ ===
    "financial": ["р╣Ар╕Зр╕┤р╕Щ", "р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в", "р╣Ар╕Зр╕┤р╕Щр╣Ар╕Фр╕╖р╕нр╕Щ", "р╕лр╕Щр╕╡р╣Й", "р╕нр╕нр╕б", "р╕ер╕Зр╕Чр╕╕р╕Щ", "р╕гр╕▓р╕Др╕▓", "р╣Бр╕Юр╕З"],
    "shopping": ["р╕Лр╕╖р╣Йр╕н", "р╕Вр╕▓р╕в", "р╕Кр╣Йр╕нр╕Ы", "р╕ер╕Ф", "р╣Вр╕Ыр╕г", "р╕Яр╕гр╕╡", "р╕кр╣Ир╕зр╕Щр╕ер╕Ф", "р╣Ар╕Лр╕е"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕кр╕╕р╕Вр╕ар╕▓р╕Ю ===
    "fitness": ["р╕нр╕нр╕Бр╕Бр╕│р╕ер╕▒р╕З", "р╕Яр╕┤р╕Х", "р╕вр╕┤р╕б", "р╕зр╕┤р╣Ир╕З", "р╕ер╕Фр╕Щр╣Йр╕│р╕лр╕Щр╕▒р╕Б", "р╕Бр╕ер╣Йр╕▓р╕бр╣Ар╕Щр╕╖р╣Йр╕н", "р╣Вр╕вр╕Др╕░"],
    "food": ["р╕нр╕▓р╕лр╕▓р╕г", "р╕Бр╕┤р╕Щ", "р╕нр╕гр╣Ир╕нр╕в", "р╕Чр╕│р╕нр╕▓р╕лр╕▓р╕г", "р╕гр╣Йр╕▓р╕Щр╕нр╕▓р╕лр╕▓р╕г", "р╣Ар╕бр╕Щр╕╣", "р╕лр╕┤р╕з"],
    
    # === р╕Ър╕гр╕┤р╕Ър╕Чр╕Бр╕▓р╕гр╣Ар╕Фр╕┤р╕Щр╕Чр╕▓р╕З ===
    "travel": ["р╣Ар╕Чр╕╡р╣Ир╕вр╕з", "р╕Чр╣Ир╕нр╕Зр╣Ар╕Чр╕╡р╣Ир╕вр╕з", "р╣Ар╕Фр╕┤р╕Щр╕Чр╕▓р╕З", "р╣Вр╕гр╕Зр╣Бр╕гр╕б", "р╕Хр╕▒р╣Йр╕зр╣Ар╕Ыр╣Зр╕Щ", "р╕Чр╕╡р╣Ир╣Ар╕Чр╕╡р╣Ир╕вр╕з", "р╕зр╕┤р╕з"]
}

# === INTENSITY PATTERNS ===
INTENSITY_PATTERNS = {
    "high": ["р╕бр╕▓р╕Б", "р╣Ар╕ер╕в", "р╕кр╕╕р╕Ф", "р╣Бр╕гр╕З", "р╕лр╕Щр╕▒р╕Б", "р╣Вр╕Др╕Хр╕г", "р╣Бр╕кр╕Щ", "р╕кр╕▓р╕лр╕▒р╕к", "р╣Ар╕Ыр╣Зр╕Щр╕Ър╣Йр╕▓", "р╕Ир╕гр╕┤р╕Зр╣Ж"],
    "medium": ["р╕Юр╕н", "р╕Др╣Ир╕нр╕Щр╕Вр╣Йр╕▓р╕З", "р╕Ыр╕▓р╕Щр╕Бр╕ер╕▓р╕З", "р╣Гр╕Кр╣Йр╣Др╕Фр╣Й", "р╣Вр╕нр╣Ар╕Д"],
    "low": ["р╣Ар╕ер╣Зр╕Бр╕Щр╣Йр╕нр╕в", "р╕Щр╕┤р╕Фр╕лр╕Щр╣Ир╕нр╕в", "р╣Ар╕Ър╕▓р╣Ж", "р╕Щр╕┤р╕Фр╣Ар╕Фр╕╡р╕вр╕з", "р╣Др╕бр╣Ир╕бр╕▓р╕Б"]
}

# --- Helper: Run yt-dlp for a single link ---
def run_ytdlp(link, outtmpl=None):
    video_id = link.split('v=')[-1].split('&')[0]
    # Always save to data/ directory
    infojson = f"data/{video_id}.info.json"
    if os.path.exists(infojson):
        return infojson
    # Ensure data/ directory exists
    if not os.path.exists('data'):
        os.makedirs('data')
    # Always use outtmpl to force yt-dlp to save in data/
    outtmpl = outtmpl or 'data/%(id)s.%(ext)s'
    cmd = [
        'python', '-m', 'yt_dlp',
        '--write-comments',
        '--skip-download',
        link,
        '-o', outtmpl
    ]
    subprocess.run(cmd, check=True)
    # Find the .info.json file in data/
    files = glob(f"data/*{video_id}*.info.json")
    return files[0] if files else None

def mask_author(author):
    if not author:
        return None
    return hashlib.sha256(author.encode('utf-8')).hexdigest()[:12]

def clean_text_privacy(text):
    if not text:
        return text
    # Mask phone numbers
    text = re.sub(r'\b\d{8,}\b', '[MASKED_PHONE]', text)
    # Mask emails
    text = re.sub(r'[\w\.-]+@[\w\.-]+', '[MASKED_EMAIL]', text)
    # Mask URLs
    text = re.sub(r'https?://\S+', '[MASKED_URL]', text)
    text = re.sub(r'www\.\S+', '[MASKED_URL]', text)
    return text

# --- Helper: Recursively flatten comments and replies ---
def flatten_comments(comments, video_id, parent_id=None, privacy_mode='none', sentiment_mode='basic', detailed_mode='single'):
    rows = []
    for c in comments:
        comment_text = c.get('text', '')
        
        # Sentiment analysis based on mode
        if sentiment_mode == 'detailed' and DETAILED_SENTIMENT_AVAILABLE:
            # р╣Гр╕Кр╣Йр╕гр╕░р╕Ър╕Ъ ML (detailed sentiment analysis)
            sentiment_result = analyze_detailed_sentiment(
                comment_text, 
                mode=detailed_mode,  # 'single' р╕лр╕гр╕╖р╕н 'multi'
                threshold=0.3,
                include_scores=True
            )
            # р╣Бр╕Ыр╕ер╕Зр╣Ар╕Ыр╣Зр╕Щ format р╕Чр╕╡р╣Ир╣Ар╕Вр╣Йр╕▓р╕Бр╕▒р╕Ъ schema р╣Ар╕Фр╕┤р╕б
            sentiment_basic = sentiment_result.get('sentiment', 'neutral')
            # confidence: р╕Цр╣Йр╕▓ ML р╕Др╕╖р╕Щ confidence р╣Гр╕лр╣Йр╣Гр╕Кр╣Й, р╕Цр╣Йр╕▓р╣Др╕бр╣Ир╕бр╕╡р╣Гр╕лр╣Йр╣Гр╕Кр╣Й max score р╕Вр╕нр╕З detailed_emotions
            confidence = sentiment_result.get('confidence')
            if confidence is None:
                # р╕ер╕нр╕Зр╕лр╕▓ max score р╕Ир╕▓р╕Б all_scores р╕Вр╕нр╕З detailed_emotions
                all_scores = sentiment_result.get('all_scores', {})
                detailed_emotions = sentiment_result.get('detailed_emotions', [])
                if detailed_emotions and all_scores:
                    confs = [all_scores.get(em, 0.0) for em in detailed_emotions if all_scores.get(em, 0.0) > 0.0]
                    confidence = max(confs) if confs else 0.0
                else:
                    confidence = 0.0
            sentiment_score = _sentiment_to_score(sentiment_basic)

        elif sentiment_mode == 'enhanced' and DETAILED_SENTIMENT_AVAILABLE:
            # р╣Гр╕Кр╣Й enhanced analysis (backward compatible)
            sentiment_result = enhanced_analyze_sentiment(comment_text)
            sentiment_basic = sentiment_result.get('sentiment', 'neutral')
            confidence = sentiment_result.get('confidence', 0.0)
            sentiment_score = sentiment_result.get('sentiment_score', 0.0)

        else:
            # р╣Гр╕Кр╣Йр╕гр╕░р╕Ър╕Ъ built-in (pattern matching) р╣Ар╕бр╕╖р╣Ир╕нр╣Др╕бр╣Ир╕бр╕╡ external modules
            sentiment_result = analyze_sentiment_builtin(
                comment_text, 
                mode=detailed_mode if sentiment_mode == 'detailed' else 'single',
                threshold=0.3
            )
            sentiment_basic = sentiment_result.get('sentiment', 'neutral')
            confidence = sentiment_result.get('confidence', 0.0)
            sentiment_score = sentiment_result.get('sentiment_score', 0.0)
        
        # Privacy handling
        author = c.get('author')
        if privacy_mode == 'mask':
            author = mask_author(author)
        elif privacy_mode == 'remove':
            author = None
        text = c.get('text')
        if privacy_mode in ('mask', 'remove'):
            text = clean_text_privacy(text)
        
        # One-hot encoding for pos/neu/neg/other
        pos = 1 if sentiment_basic == 'positive' else 0
        neu = 1 if sentiment_basic == 'neutral' else 0
        neg = 1 if sentiment_basic == 'negative' else 0
        other = 1 if sentiment_basic not in ('positive', 'neutral', 'negative') else 0
        # Base row structure
        row = {
            'video_id': video_id,
            'comment_id': c.get('id'),
            'parent_id': parent_id,
            'author': author,
            'text': text,
            'like_count': c.get('like_count'),
            'published': c.get('published'),
            'is_reply': parent_id is not None,
            # --- р╕бр╕▓р╕Хр╕гр╕Рр╕▓р╕Щ sentiment schema ---
            'sentiment': sentiment_basic,
            'confidence': confidence,
            'sentiment_score': sentiment_score,
            'pos': pos,
            'neu': neu,
            'neg': neg,
            'other': other,
            'model_type': sentiment_result.get('model_type', 'unknown'),
            'privacy_notice': 'This dataset is for research only. Do not use for commercial or personal identification.'
        }
        
        # р╣Ар╕Юр╕┤р╣Ир╕бр╕Вр╣Йр╕нр╕бр╕╣р╕е detailed sentiment р╕Цр╣Йр╕▓р╣Ар╕Ыр╕┤р╕Фр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ
        if sentiment_mode == 'detailed':
            if DETAILED_SENTIMENT_AVAILABLE:
                # р╣Гр╕Кр╣Йр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕▓р╕Б external detailed sentiment module
                row.update({
                    'detailed_sentiment_analysis': sentiment_result,
                    'analysis_mode': detailed_mode,
                    'detailed_emotions': sentiment_result.get('detailed_emotions', []) if detailed_mode == 'multi' else [sentiment_result.get('detailed_emotion', '')],
                    'emotion_groups': sentiment_result.get('emotion_groups', []) if detailed_mode == 'multi' else [sentiment_result.get('emotion_group', '')],
                    'context': sentiment_result.get('context', 'unknown')
                })
            else:
                # р╣Гр╕Кр╣Й built-in patterns
                if detailed_mode == 'multi':
                    row.update({
                        'detailed_sentiment_analysis': sentiment_result,
                        'analysis_mode': detailed_mode,
                        'detailed_emotions': sentiment_result.get('detailed_emotions', []),
                        'emotion_groups': sentiment_result.get('emotion_groups', []),
                        'context': sentiment_result.get('context', 'unknown')
                    })
                else:
                    row.update({
                        'detailed_sentiment_analysis': sentiment_result,
                        'analysis_mode': detailed_mode,
                        'detailed_emotions': [sentiment_result.get('detailed_emotion', '')],
                        'emotion_groups': [sentiment_result.get('emotion_group', '')],
                        'context': sentiment_result.get('context', 'unknown')
                    })
        
        elif sentiment_mode == 'enhanced':
            if DETAILED_SENTIMENT_AVAILABLE:
                # р╣Гр╕Кр╣Йр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕▓р╕Б external enhanced module
                row.update({
                    'detailed_emotion': sentiment_result.get('detailed_emotion', ''),
                    'emotion_group': sentiment_result.get('emotion_group', ''),
                    'context': sentiment_result.get('context', 'unknown')
                })
            else:
                # р╣Гр╕Кр╣Й built-in patterns р╕кр╕│р╕лр╕гр╕▒р╕Ъ enhanced mode
                row.update({
                    'detailed_emotion': sentiment_result.get('detailed_emotion', ''),
                    'emotion_group': sentiment_result.get('emotion_group', ''),
                    'context': sentiment_result.get('context', 'unknown')
                })
        
        rows.append(row)
        
        # Recursively add replies
        replies = c.get('replies', [])
        if replies:
            rows.extend(flatten_comments(replies, video_id, parent_id=c.get('id'), privacy_mode=privacy_mode, sentiment_mode=sentiment_mode, detailed_mode=detailed_mode))
    return rows

def _sentiment_to_score(sentiment: str) -> float:
    """р╣Бр╕Ыр╕ер╕З sentiment р╣Ар╕Ыр╣Зр╕Щ score р╕кр╕│р╕лр╕гр╕▒р╕Ъ backward compatibility"""
    if sentiment == "positive":
        return 0.7
    elif sentiment == "negative":
        return -0.7
    else:
        return 0.0

# === BUILT-IN SENTIMENT ANALYSIS FUNCTIONS ===

def extract_emojis(text):
    """р╕Фр╕╢р╕З emojis р╕Ир╕▓р╕Бр╕Вр╣Йр╕нр╕Др╕зр╕▓р╕б"""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map
        "\U0001F1E0-\U0001F1FF"  # flags
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE
    )
    return emoji_pattern.findall(text)

def calculate_emotion_scores(text):
    """р╕Др╕│р╕Щр╕зр╕Ур╕Др╕░р╣Бр╕Щр╕Щр╕кр╕│р╕лр╕гр╕▒р╕Ър╣Бр╕Хр╣Ир╕ер╕░р╕нр╕▓р╕гр╕бр╕Ур╣М"""
    if not text:
        return {}
    
    clean_text = text.lower()
    emojis = extract_emojis(text)
    emotion_scores = {}
    
    for emotion, config in EMOTION_PATTERNS.items():
        score = 0.0
        
        # р╕Др╕░р╣Бр╕Щр╕Щр╕Ир╕▓р╕Бр╕Др╕│р╕кр╕│р╕Др╕▒р╕Н
        for keyword in config["keywords"]:
            if keyword in clean_text:
                score += 1.0
        
        # р╕Др╕░р╣Бр╕Щр╕Щр╕Ир╕▓р╕Б patterns (regex)
        for pattern in config.get("patterns", []):
            matches = re.findall(pattern, clean_text)
            score += len(matches) * 1.5
        
        # р╕Др╕░р╣Бр╕Щр╕Щр╕Ир╕▓р╕Б emojis
        for emoji in emojis:
            if emoji in config.get("emojis", []):
                score += 2.0
        
        # р╕Ыр╕гр╕▒р╕Ър╕Др╕░р╣Бр╕Щр╕Щр╕Хр╕▓р╕бр╕Др╕зр╕▓р╕бр╣Ар╕Вр╣Йр╕бр╕Вр╣Йр╕Щ
        intensity_bonus = calculate_intensity_bonus(clean_text)
        score *= (1 + intensity_bonus)
        
        emotion_scores[emotion] = min(score, 5.0)  # р╕Ир╕│р╕Бр╕▒р╕Фр╕Др╕░р╣Бр╕Щр╕Щр╕кр╕╣р╕Зр╕кр╕╕р╕Ф
    
    return emotion_scores

def calculate_intensity_bonus(text):
    """р╕Др╕│р╕Щр╕зр╕У bonus р╕Ир╕▓р╕Бр╕Др╕зр╕▓р╕бр╣Ар╕Вр╣Йр╕бр╕Вр╣Йр╕Щр╕Вр╕нр╕Зр╕Бр╕▓р╕гр╣Бр╕кр╕Фр╕Зр╕нр╕нр╕Б"""
    bonus = 0.0
    
    for intensity, words in INTENSITY_PATTERNS.items():
        for word in words:
            if word in text:
                if intensity == "high":
                    bonus += 0.5
                elif intensity == "medium":
                    bonus += 0.2
                elif intensity == "low":
                    bonus += 0.1
    
    return min(bonus, 1.0)  # р╕Ир╕│р╕Бр╕▒р╕Ф bonus р╕кр╕╣р╕Зр╕кр╕╕р╕Ф

def normalize_scores(scores):
    """normalize р╕Др╕░р╣Бр╕Щр╕Щр╣Гр╕лр╣Йр╕нр╕вр╕╣р╣Ир╣Гр╕Щр╕Кр╣Ир╕зр╕З 0-1"""
    if not scores:
        return {}
    
    max_score = max(scores.values())
    if max_score == 0:
        return scores
    
    return {emotion: score / max_score for emotion, score in scores.items()}

def determine_context(text):
    """р╕Бр╕│р╕лр╕Щр╕Фр╕Ър╕гр╕┤р╕Ър╕Чр╕Бр╕▓р╕гр╣Гр╕Кр╣Йр╕ар╕▓р╕йр╕▓р╣Бр╕Ър╕Ър╕Др╕гр╕нр╕Ър╕Др╕ер╕╕р╕б"""
    if not text:
        return {
            "primary_context": "neutral",
            "all_contexts": {},
            "formality_level": "neutral",
            "social_setting": "general",
            "emotional_tone": "neutral",
            "generation": "unknown",
            "profession": "general"
        }
    
    clean_text = text.lower()
    context_scores = {}
    
    # р╕Др╕│р╕Щр╕зр╕Ур╕Др╕░р╣Бр╕Щр╕Щр╕кр╕│р╕лр╕гр╕▒р╕Ър╣Бр╕Хр╣Ир╕ер╕░р╕Ър╕гр╕┤р╕Ър╕Ч
    for context, words in CONTEXT_PATTERNS.items():
        score = 0
        for word in words:
            if word in clean_text:
                score += 1
        if score > 0:
            context_scores[context] = score
    
    if not context_scores:
        return {
            "primary_context": "neutral",
            "all_contexts": {},
            "formality_level": "neutral",
            "social_setting": "general",
            "emotional_tone": "neutral",
            "generation": "unknown",
            "profession": "general"
        }
    
    # р╕лр╕▓р╕Ър╕гр╕┤р╕Ър╕Чр╕лр╕ер╕▒р╕Б
    primary_context = max(context_scores, key=context_scores.get)
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕гр╕░р╕Фр╕▒р╕Ър╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╕Чр╕▓р╕Зр╕Бр╕▓р╕г
    formality_score = {
        "formal": context_scores.get("formal", 0),
        "informal": context_scores.get("informal", 0),
        "slang": context_scores.get("slang", 0)
    }
    formality_level = max(formality_score, key=formality_score.get) if any(formality_score.values()) else "neutral"
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕Бр╕▓р╕гр╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╕Чр╕▓р╕Зр╕кр╕▒р╕Зр╕Др╕б
    social_contexts = ["social_media", "news_media", "review", "business", "education", "healthcare", "government"]
    social_scores = {ctx: context_scores.get(ctx, 0) for ctx in social_contexts}
    social_setting = max(social_scores, key=social_scores.get) if any(social_scores.values()) else "general"
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Вр╕Чр╕Щр╕нр╕▓р╕гр╕бр╕Ур╣М
    emotional_contexts = ["complaint", "praise", "emergency", "celebration", "condolence", "question"]
    emotional_scores = {ctx: context_scores.get(ctx, 0) for ctx in emotional_contexts}
    emotional_tone = max(emotional_scores, key=emotional_scores.get) if any(emotional_scores.values()) else "neutral"
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕гр╕╕р╣Ир╕Щ/р╕нр╕▓р╕вр╕╕
    generation_contexts = ["gen_z", "millennial", "gen_x"]
    generation_scores = {ctx: context_scores.get(ctx, 0) for ctx in generation_contexts}
    generation = max(generation_scores, key=generation_scores.get) if any(generation_scores.values()) else "unknown"
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕зр╕┤р╕Кр╕▓р╕Кр╕╡р╕Ю
    profession_contexts = ["business", "education", "healthcare", "government", "tech", "gaming"]
    profession_scores = {ctx: context_scores.get(ctx, 0) for ctx in profession_contexts}
    profession = max(profession_scores, key=profession_scores.get) if any(profession_scores.values()) else "general"
    
    return {
        "primary_context": primary_context,
        "all_contexts": context_scores,
        "formality_level": formality_level,
        "social_setting": social_setting,
        "emotional_tone": emotional_tone,
        "generation": generation,
        "profession": profession,
        "context_confidence": round(context_scores[primary_context] / len(clean_text.split()) if clean_text.split() else 0, 3)
    }

def analyze_sentiment_builtin(text, mode='single', threshold=0.3):
    """р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М sentiment р╣Бр╕Ър╕Ъ built-in (р╣Др╕бр╣Ир╕Хр╣Йр╕нр╕Зр╕Юр╕╢р╣Ир╕Зр╕Юр╕▓р╣Др╕Яр╕ер╣Мр╕нр╕╖р╣Ир╕Щ)"""
    if not text or not text.strip():
        return {
            "sentiment": "neutral",
            "confidence": 0.0,
            "sentiment_score": 0.0,
            "detailed_emotion": "р╣Ар╕Йр╕в р╣Ж",
            "emotion_group": "Neutral",
            "context": {
                "primary_context": "neutral",
                "formality_level": "neutral",
                "social_setting": "general",
                "emotional_tone": "neutral"
            },
            "model_type": "builtin_pattern_matching"
        }
    
    # р╕Др╕│р╕Щр╕зр╕Ур╕Др╕░р╣Бр╕Щр╕Щр╕кр╕│р╕лр╕гр╕▒р╕Ър╣Бр╕Хр╣Ир╕ер╕░р╕нр╕▓р╕гр╕бр╕Ур╣М
    raw_scores = calculate_emotion_scores(text)
    normalized_scores = normalize_scores(raw_scores)
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕Ър╕гр╕┤р╕Ър╕Чр╣Бр╕Ър╕Ър╕Др╕гр╕нр╕Ър╕Др╕ер╕╕р╕б
    context_data = determine_context(text)
    context_patterns = analyze_context_patterns(text)
    context_insights = get_context_insights(context_data)
    
    if mode == 'single':
        # Single label mode
        if not normalized_scores or max(normalized_scores.values(), default=0.0) == 0.0:
            predicted_label = "р╣Ар╕Йр╕в р╣Ж"
            confidence = 0.0
        else:
            predicted_label = max(normalized_scores, key=normalized_scores.get)
            # р╕Цр╣Йр╕▓р╕Др╕░р╣Бр╕Щр╕Щр╕кр╕╣р╕Зр╕кр╕╕р╕Фр╣Ар╕Ыр╣Зр╕Щ 0.0 р╣Гр╕лр╣Йр╕Цр╕╖р╕нр╕зр╣Ир╕▓р╣Ар╕Ыр╣Зр╕Щ neutral
            if normalized_scores[predicted_label] == 0.0:
                predicted_label = "р╣Ар╕Йр╕в р╣Ж"
                confidence = 0.0
            else:
                confidence = normalized_scores[predicted_label]

        # р╕Бр╕│р╕лр╕Щр╕Фр╕Бр╕ер╕╕р╣Ир╕бр╣Бр╕ер╕░ sentiment р╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ
        group = LABEL_TO_GROUP.get(predicted_label, "Neutral")
        basic_sentiment = "positive" if group == "Positive" else "negative" if group == "Negative" else "neutral"
        sentiment_score = _sentiment_to_score(basic_sentiment)

        result = {
            "sentiment": basic_sentiment,
            "confidence": round(confidence, 3),
            "sentiment_score": sentiment_score,
            "detailed_emotion": predicted_label,
            "emotion_group": group,
            "context": context_data,
            "context_patterns": context_patterns,
            "context_insights": context_insights,
            "scores": {k: round(v, 3) for k, v in normalized_scores.items()},
            "model_type": "builtin_pattern_matching"
        }

    else:  # multi label mode
        # Multi-label mode
        predicted_labels = []
        for emotion, score in normalized_scores.items():
            if score >= threshold:
                predicted_labels.append(emotion)

        # р╕Цр╣Йр╕▓р╣Др╕бр╣Ир╕бр╕╡ label р╕Чр╕╡р╣Ир╕Ьр╣Ир╕▓р╕Щ threshold р╕лр╕гр╕╖р╕нр╕Чр╕╕р╕Бр╕Др╣Ир╕▓р╣Ар╕Ыр╣Зр╕Щ 0.0 р╣Гр╕лр╣Й default р╣Ар╕Ыр╣Зр╕Щ "р╣Ар╕Йр╕в р╣Ж"
        if (not predicted_labels) or all(normalized_scores.get(label, 0.0) == 0.0 for label in predicted_labels):
            predicted_labels = ["р╣Ар╕Йр╕в р╣Ж"]

        # р╕Бр╕│р╕лр╕Щр╕Фр╕Бр╕ер╕╕р╣Ир╕бр╣Бр╕ер╕░ sentiment р╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ
        groups = list(set(LABEL_TO_GROUP.get(label, "Neutral") for label in predicted_labels))

        # р╕Бр╕│р╕лр╕Щр╕Ф basic sentiment р╕Ир╕▓р╕Бр╕Бр╕ер╕╕р╣Ир╕бр╕лр╕ер╕▒р╕Б
        if "Positive" in groups:
            basic_sentiment = "positive"
        elif "Negative" in groups:
            basic_sentiment = "negative"
        else:
            basic_sentiment = "neutral"

        sentiment_score = _sentiment_to_score(basic_sentiment)
        # confidence = max р╕Вр╕нр╕З label р╕Чр╕╡р╣Ир╣Др╕бр╣Ир╣Гр╕Кр╣И 0.0 р╕Цр╣Йр╕▓р╕бр╕╡
        nonzero_conf = [normalized_scores.get(label, 0.0) for label in predicted_labels if normalized_scores.get(label, 0.0) > 0.0]
        max_confidence = max(nonzero_conf) if nonzero_conf else 0.0

        result = {
            "sentiment": basic_sentiment,
            "confidence": round(max_confidence, 3),
            "sentiment_score": sentiment_score,
            "detailed_emotions": predicted_labels,
            "emotion_groups": groups,
            "context": context_data,
            "context_patterns": context_patterns,
            "context_insights": context_insights,
            "scores": {k: round(v, 3) for k, v in normalized_scores.items()},
            "threshold": threshold,
            "model_type": "builtin_pattern_matching"
        }

    return result

# --- Main ---

# --- Move show_sentiment_statistics above main() to avoid NameError ---
def show_sentiment_statistics(rows, detailed_mode):
    """Show summary statistics for sentiment analysis"""
    if not rows:
        return
    print(f"\nЁЯУК Sentiment Analysis Statistics:")
    print(f"   Total comments analyzed: {len(rows)}")
    print(f"   Analysis mode: {detailed_mode}")
    # Basic sentiment counts
    sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
    emotion_counts = {}
    group_counts = {}
    context_counts = {
        "primary_context": {},
        "formality_level": {},
        "social_setting": {},
        "emotional_tone": {},
        "generation": {},
        "profession": {}
    }
    for row in rows:
        # Basic sentiment
        sentiment = row.get('sentiment', 'neutral')
        if sentiment in sentiment_counts:
            sentiment_counts[sentiment] += 1
        # Detailed emotions (if available)
        if detailed_mode == 'single':
            emotions = row.get('detailed_emotions', [])
            if not emotions:
                emotion = row.get('detailed_emotion', '')
                emotions = [emotion] if emotion else ['']
            groups = row.get('emotion_groups', [])
            if not groups:
                group = row.get('emotion_group', '')
                groups = [group] if group else ['']
        else:  # multi
            emotions = row.get('detailed_emotions', [])
            groups = row.get('emotion_groups', [])
        for emotion in emotions:
            if emotion and emotion.strip():
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        for group in groups:
            if group and group.strip():
                group_counts[group] = group_counts.get(group, 0) + 1
        # Context analysis (р╕гр╕нр╕Зр╕гр╕▒р╕Ър╕Чр╕▒р╣Йр╕Зр╣Бр╕Ър╕Ър╣Ар╕Бр╣Ир╕▓р╣Бр╕ер╕░р╣Гр╕лр╕бр╣И)
        context = row.get('context', {})
        if isinstance(context, dict):
            for context_type in context_counts:
                if context_type in context:
                    value = context[context_type]
                    if value and str(value).strip():
                        context_counts[context_type][value] = context_counts[context_type].get(value, 0) + 1
        elif isinstance(context, str):
            if context and context.strip():
                context_counts["primary_context"][context] = context_counts["primary_context"].get(context, 0) + 1
    print(f"   Basic sentiment distribution:")
    for sentiment, count in sentiment_counts.items():
        percentage = (count / len(rows)) * 100
        print(f"     {sentiment}: {count} ({percentage:.1f}%)")
    if emotion_counts:
        print(f"   Top emotions detected:")
        sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:8]
        for emotion, count in sorted_emotions:
            percentage = (count / len(rows)) * 100
            print(f"     {emotion}: {count} ({percentage:.1f}%)")
    if group_counts:
        print(f"   Emotion group distribution:")
        for group, count in group_counts.items():
            percentage = (count / len(rows)) * 100
            print(f"     {group}: {count} ({percentage:.1f}%)")
    print(f"   ЁЯУЛ Context Analysis:")
    if context_counts["formality_level"]:
        print(f"     Formality levels:")
        for level, count in context_counts["formality_level"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {level}: {count} ({percentage:.1f}%)")
    if context_counts["social_setting"]:
        print(f"     Social settings:")
        sorted_social = sorted(context_counts["social_setting"].items(), key=lambda x: x[1], reverse=True)[:5]
        for setting, count in sorted_social:
            percentage = (count / len(rows)) * 100
            print(f"       {setting}: {count} ({percentage:.1f}%)")
    if context_counts["emotional_tone"]:
        print(f"     Emotional tones:")
        sorted_tones = sorted(context_counts["emotional_tone"].items(), key=lambda x: x[1], reverse=True)[:5]
        for tone, count in sorted_tones:
            percentage = (count / len(rows)) * 100
            print(f"       {tone}: {count} ({percentage:.1f}%)")
    if context_counts["generation"]:
        print(f"     Generational markers:")
        for generation, count in context_counts["generation"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {generation}: {count} ({percentage:.1f}%)")
    if context_counts["profession"]:
        print(f"     Professional contexts:")
        sorted_professions = sorted(context_counts["profession"].items(), key=lambda x: x[1], reverse=True)[:5]
        for profession, count in sorted_professions:
            percentage = (count / len(rows)) * 100
            print(f"       {profession}: {count} ({percentage:.1f}%)")
    if context_counts["primary_context"]:
        print(f"     Primary contexts:")
        sorted_contexts = sorted(context_counts["primary_context"].items(), key=lambda x: x[1], reverse=True)[:5]
        for context, count in sorted_contexts:
            percentage = (count / len(rows)) * 100
            print(f"       {context}: {count} ({percentage:.1f}%)")
    model_types = {}
    for row in rows:
        model_type = row.get('model_type', 'unknown')
        model_types[model_type] = model_types.get(model_type, 0) + 1
    if model_types:
        print(f"   Model types used:")
        for model, count in model_types.items():
            percentage = (count / len(rows)) * 100
            print(f"     {model}: {count} ({percentage:.1f}%)")
    print()

def main():
    parser = argparse.ArgumentParser(description='Extract YouTube comments using yt-dlp and export as JSONL with advanced Thai sentiment analysis.')
    parser.add_argument('--links', default='youtube_real_links_1500.txt', help='Text file with YouTube links (one per line)')
    parser.add_argument('--output', default='youtube_comments.jsonl', help='Output JSONL file')
    parser.add_argument('--privacy', default='none', choices=['none', 'mask', 'remove'], help='Privacy mode: mask (hash author, mask PII), remove (no author, mask PII), none (no privacy)')
    # Sentiment analysis options
    parser.add_argument('--sentiment-mode', default='basic', 
                       choices=['basic', 'enhanced', 'detailed'], 
                       help='Sentiment analysis mode: basic (old system), enhanced (new system, backward compatible), detailed (full multi-emotion analysis)')
    parser.add_argument('--detailed-mode', default='single', 
                       choices=['single', 'multi'],
                       help='For detailed sentiment: single (single-label classification) or multi (multi-label classification)')
    parser.add_argument('--no-sentiment', action='store_true', 
                       help='Disable sentiment analysis completely')
    import sys
    if len(sys.argv) == 1:
        parser.print_help()
        print("\n[ERROR] р╕Бр╕гр╕╕р╕Ур╕▓р╕гр╕░р╕Ър╕╕ arguments р╕Чр╕╡р╣Ир╕Ир╕│р╣Ар╕Ыр╣Зр╕Щ р╣Ар╕Кр╣Ир╕Щ --links <р╣Др╕Яр╕ер╣Мр╕ер╕┤р╕Зр╕Бр╣М YouTube> р╕лр╕гр╕╖р╕нр╕Фр╕╣р╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╕Бр╕▓р╕гр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щр╕Фр╣Йр╕▓р╕Щр╕Ър╕Щ\n")
        sys.exit(1)
    try:
        args = parser.parse_args()
    except SystemExit as e:
        # argparse throws SystemExit on error, catch and print friendlier message
        if e.code != 0:
            print("\n[ERROR] р╕Юр╕Ъ argument р╕Чр╕╡р╣Ир╣Др╕бр╣Ир╕Цр╕╣р╕Бр╕Хр╣Йр╕нр╕З р╕лр╕гр╕╖р╕нр╕бр╕╡ argument р╣Бр╕Ыр╕ер╕Бр╕Ыр╕ер╕нр╕б\nр╕Бр╕гр╕╕р╕Ур╕▓р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕Др╕│р╕кр╕▒р╣Ир╕Зр╣Бр╕ер╕░р╕Фр╕╣р╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╕Бр╕▓р╕гр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щр╕Фр╣Йр╕▓р╕Щр╕Ър╕Щ\n")
        raise
    # --- Main processing logic ---
    # 1. Load YouTube links
    try:
        with open(args.links, encoding='utf-8') as f:
            links = [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"[ERROR] р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╣Ар╕Ыр╕┤р╕Фр╣Др╕Яр╕ер╣Мр╕ер╕┤р╕Зр╕Бр╣М: {args.links} : {e}")
        return
    # --- р╣Ар╕Юр╕┤р╣Ир╕бр╣Вр╕лр╕бр╕Ф: р╣Гр╕Кр╣Йр╣Др╕Яр╕ер╣М .info.json р╕Чр╕╡р╣Ир╕бр╕╡р╕нр╕вр╕╣р╣Ир╣Бр╕ер╣Йр╕з р╕лр╕гр╕╖р╕н process р╣Гр╕лр╕бр╣И ---
    all_rows = []
    for link in tqdm(links, desc="Processing YouTube links"):
        try:
            # р╕Фр╕╢р╕З video_id р╕Ир╕▓р╕Бр╕ер╕┤р╕Зр╕Бр╣М
            video_id = link.split('v=')[-1].split('&')[0]
            # р╕лр╕▓р╣Др╕Яр╕ер╣М .info.json р╕Чр╕╡р╣Ир╕бр╕╡р╕нр╕вр╕╣р╣Ир╣Гр╕Щ data/ р╕лр╕гр╕╖р╕нр╣Вр╕Яр╕ер╣Ар╕Фр╕нр╕гр╣Мр╕Ыр╕▒р╕Ир╕Ир╕╕р╕Ър╕▒р╕Щ
            infojson_candidates = [
                f"data/{video_id}.info.json",
                f"{video_id}.info.json"
            ]
            infojson = None
            for candidate in infojson_candidates:
                if os.path.exists(candidate):
                    infojson = candidate
                    break
            # р╕Цр╣Йр╕▓р╣Др╕бр╣Ир╣Ар╕Ир╕н р╣Гр╕лр╣Йр╕гр╕▒р╕Щ yt-dlp р╣Ар╕Юр╕╖р╣Ир╕нр╣Вр╕лр╕ер╕Фр╣Гр╕лр╕бр╣И
            if not infojson:
                infojson = run_ytdlp(link)
                if not infojson or not os.path.exists(infojson):
                    print(f"[WARN] р╣Др╕бр╣Ир╕Юр╕Ър╣Др╕Яр╕ер╣Мр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕кр╕│р╕лр╕гр╕▒р╕Ъ {link}")
                    continue
            with open(infojson, encoding='utf-8') as jf:
                info = json.load(jf)
            comments = info.get('comments', [])
            video_id = info.get('id', video_id)
            if args.no_sentiment:
                rows = flatten_comments_no_sentiment(comments, video_id, privacy_mode=args.privacy)
            else:
                rows = flatten_comments(
                    comments, video_id,
                    privacy_mode=args.privacy,
                    sentiment_mode=args.sentiment_mode,
                    detailed_mode=args.detailed_mode
                )
            all_rows.extend(rows)
        except Exception as e:
            print(f"[ERROR] р╕ер╣Йр╕бр╣Ар╕лр╕ер╕зр╕Чр╕╡р╣Ир╕ер╕┤р╕Зр╕Бр╣М {link}: {e}")
    # 4. Write output
    try:
        with open(args.output, 'w', encoding='utf-8') as out:
            for row in all_rows:
                out.write(json.dumps(row, ensure_ascii=False) + '\n')
        print(f"\n[INFO] р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣Мр╕Чр╕╡р╣И {args.output} р╣Бр╕ер╣Йр╕з р╕Ир╕│р╕Щр╕зр╕Щ {len(all_rows)} records")
    except Exception as e:
        print(f"[ERROR] р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╣Ар╕Вр╕╡р╕вр╕Щр╣Др╕Яр╕ер╣М output: {args.output} : {e}")
        return
    # 5. Show statistics (optional)
    if not args.no_sentiment:
        show_sentiment_statistics(all_rows, args.detailed_mode)

# --- Main entry point ---
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[FATAL ERROR] р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╕гр╕▒р╕Щр╣Вр╕Ыр╕гр╣Бр╕Бр╕гр╕бр╣Др╕Фр╣Й: ", e)
        import sys
        sys.exit(1)

def flatten_comments_no_sentiment(comments, video_id, parent_id=None, privacy_mode='none'):
    """Flatten comments without sentiment analysis (for performance/testing)"""
    rows = []
    for c in comments:
        # Privacy handling
        author = c.get('author')
        if privacy_mode == 'mask':
            author = mask_author(author)
        elif privacy_mode == 'remove':
            author = None
        text = c.get('text')
        if privacy_mode in ('mask', 'remove'):
            text = clean_text_privacy(text)
        
        # Default: all 0, only neu=1 (since no sentiment analysis)
        row = {
            'video_id': video_id,
            'comment_id': c.get('id'),
            'parent_id': parent_id,
            'author': author,
            'text': text,
            'like_count': c.get('like_count'),
            'published': c.get('published'),
            'is_reply': parent_id is not None,
            'pos': 0,
            'neu': 1,
            'neg': 0,
            'privacy_notice': 'This dataset is for research only. Do not use for commercial or personal identification.'
        }
        rows.append(row)
        
        # Recursively add replies
        replies = c.get('replies', [])
        if replies:
            rows.extend(flatten_comments_no_sentiment(replies, video_id, parent_id=c.get('id'), privacy_mode=privacy_mode))
    return rows

def show_sentiment_statistics(rows, detailed_mode):
    """Show summary statistics for sentiment analysis"""
    if not rows:
        return
    
    print(f"\nЁЯУК Sentiment Analysis Statistics:")
    print(f"   Total comments analyzed: {len(rows)}")
    print(f"   Analysis mode: {detailed_mode}")
    
    # Basic sentiment counts
    sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
    emotion_counts = {}
    group_counts = {}
    context_counts = {
        "primary_context": {},
        "formality_level": {},
        "social_setting": {},
        "emotional_tone": {},
        "generation": {},
        "profession": {}
    }
    
    for row in rows:
        # Basic sentiment
        sentiment = row.get('sentiment', 'neutral')
        if sentiment in sentiment_counts:
            sentiment_counts[sentiment] += 1
        
        # Detailed emotions (if available)
        if detailed_mode == 'single':
            # р╕Фр╕╢р╕Зр╕нр╕▓р╕гр╕бр╕Ур╣Мр╕Ир╕▓р╕Б detailed_emotions array р╕лр╕гр╕╖р╕н detailed_emotion field
            emotions = row.get('detailed_emotions', [])
            if not emotions:
                emotion = row.get('detailed_emotion', '')
                emotions = [emotion] if emotion else ['']
            
            groups = row.get('emotion_groups', [])
            if not groups:
                group = row.get('emotion_group', '')
                groups = [group] if group else ['']
        else:  # multi
            emotions = row.get('detailed_emotions', [])
            groups = row.get('emotion_groups', [])
        
        for emotion in emotions:
            if emotion and emotion.strip():
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        for group in groups:
            if group and group.strip():
                group_counts[group] = group_counts.get(group, 0) + 1
        
        # Context analysis (р╕гр╕нр╕Зр╕гр╕▒р╕Ър╕Чр╕▒р╣Йр╕Зр╣Бр╕Ър╕Ър╣Ар╕Бр╣Ир╕▓р╣Бр╕ер╕░р╣Гр╕лр╕бр╣И)
        context = row.get('context', {})
        if isinstance(context, dict):
            # р╣Бр╕Ър╕Ър╣Гр╕лр╕бр╣Ир╕Чр╕╡р╣Ир╕ер╕░р╣Ар╕нр╕╡р╕вр╕Ф
            for context_type in context_counts:
                if context_type in context:
                    value = context[context_type]
                    if value and str(value).strip():
                        context_counts[context_type][value] = context_counts[context_type].get(value, 0) + 1
        elif isinstance(context, str):
            # р╣Бр╕Ър╕Ър╣Ар╕Бр╣Ир╕▓
            if context and context.strip():
                context_counts["primary_context"][context] = context_counts["primary_context"].get(context, 0) + 1
    
    print(f"   Basic sentiment distribution:")
    for sentiment, count in sentiment_counts.items():
        percentage = (count / len(rows)) * 100
        print(f"     {sentiment}: {count} ({percentage:.1f}%)")
    
    if emotion_counts:
        print(f"   Top emotions detected:")
        sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:8]
        for emotion, count in sorted_emotions:
            percentage = (count / len(rows)) * 100
            print(f"     {emotion}: {count} ({percentage:.1f}%)")
    
    if group_counts:
        print(f"   Emotion group distribution:")
        for group, count in group_counts.items():
            percentage = (count / len(rows)) * 100
            print(f"     {group}: {count} ({percentage:.1f}%)")
    
    # р╣Бр╕кр╕Фр╕Зр╕кр╕Цр╕┤р╕Хр╕┤р╕Ър╕гр╕┤р╕Ър╕Чр╣Бр╕Ър╕Ър╕ер╕░р╣Ар╕нр╕╡р╕вр╕Ф
    print(f"   ЁЯУЛ Context Analysis:")
    
    # р╕гр╕░р╕Фр╕▒р╕Ър╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╕Чр╕▓р╕Зр╕Бр╕▓р╕г
    if context_counts["formality_level"]:
        print(f"     Formality levels:")
        for level, count in context_counts["formality_level"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {level}: {count} ({percentage:.1f}%)")
    
    # р╕Бр╕▓р╕гр╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╕Чр╕▓р╕Зр╕кр╕▒р╕Зр╕Др╕б
    if context_counts["social_setting"]:
        print(f"     Social settings:")
        sorted_social = sorted(context_counts["social_setting"].items(), key=lambda x: x[1], reverse=True)[:5]
        for setting, count in sorted_social:
            percentage = (count / len(rows)) * 100
            print(f"       {setting}: {count} ({percentage:.1f}%)")
    
    # р╣Вр╕Чр╕Щр╕нр╕▓р╕гр╕бр╕Ур╣М
    if context_counts["emotional_tone"]:
        print(f"     Emotional tones:")
        sorted_tones = sorted(context_counts["emotional_tone"].items(), key=lambda x: x[1], reverse=True)[:5]
        for tone, count in sorted_tones:
            percentage = (count / len(rows)) * 100
            print(f"       {tone}: {count} ({percentage:.1f}%)")
    
    # р╕гр╕╕р╣Ир╕Щ/р╕нр╕▓р╕вр╕╕
    if context_counts["generation"]:
        print(f"     Generational markers:")
        for generation, count in context_counts["generation"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {generation}: {count} ({percentage:.1f}%)")
    
    # р╕зр╕┤р╕Кр╕▓р╕Кр╕╡р╕Ю
    if context_counts["profession"]:
        print(f"     Professional contexts:")
        sorted_professions = sorted(context_counts["profession"].items(), key=lambda x: x[1], reverse=True)[:5]
        for profession, count in sorted_professions:
            percentage = (count / len(rows)) * 100
            print(f"       {profession}: {count} ({percentage:.1f}%)")
    
    # р╕Ър╕гр╕┤р╕Ър╕Чр╕лр╕ер╕▒р╕Б (fallback р╕кр╕│р╕лр╕гр╕▒р╕Ър╣Бр╕Ър╕Ър╣Ар╕Бр╣Ир╕▓)
    if context_counts["primary_context"]:
        print(f"     Primary contexts:")
        sorted_contexts = sorted(context_counts["primary_context"].items(), key=lambda x: x[1], reverse=True)[:5]
        for context, count in sorted_contexts:
            percentage = (count / len(rows)) * 100
            print(f"       {context}: {count} ({percentage:.1f}%)")
    
    # р╣Бр╕кр╕Фр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Ар╕Бр╕╡р╣Ир╕вр╕зр╕Бр╕▒р╕Ъ model р╕Чр╕╡р╣Ир╣Гр╕Кр╣Й
    model_types = {}
    for row in rows:
        model_type = row.get('model_type', 'unknown')
        model_types[model_type] = model_types.get(model_type, 0) + 1
    
    if model_types:
        print(f"   Model types used:")
        for model, count in model_types.items():
            percentage = (count / len(rows)) * 100
            print(f"     {model}: {count} ({percentage:.1f}%)")
    
    print()

def analyze_context_patterns(text):
    """р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕гр╕╣р╕Ыр╣Бр╕Ър╕Ър╕Бр╕▓р╕гр╣Гр╕Кр╣Йр╕ар╕▓р╕йр╕▓р╣Ар╕Кр╕┤р╕Зр╕ер╕╢р╕Б"""
    if not text:
        return {}
    
    clean_text = text.lower()
    patterns = {
        "communication_style": {},
        "relationship_level": {},
        "topic_category": {},
        "cultural_elements": {},
        "generational_markers": {},
        "emotional_indicators": {}
    }
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕гр╕╣р╕Ыр╣Бр╕Ър╕Ър╕Бр╕▓р╕гр╕кр╕╖р╣Ир╕нр╕кр╕▓р╕г
    comm_styles = {
        "formal": CONTEXT_PATTERNS["formal"],
        "informal": CONTEXT_PATTERNS["informal"], 
        "slang": CONTEXT_PATTERNS["slang"]
    }
    
    for style, words in comm_styles.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["communication_style"][style] = count
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕гр╕░р╕Фр╕▒р╕Ър╕Др╕зр╕▓р╕бр╕кр╕▒р╕бр╕Юр╕▒р╕Щр╕Шр╣М
    relationship_levels = {
        "intimate": CONTEXT_PATTERNS["intimate"],
        "friendly": CONTEXT_PATTERNS["friendly"],
        "personal": CONTEXT_PATTERNS["personal"]
    }
    
    for level, words in relationship_levels.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["relationship_level"][level] = count
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕лр╕бр╕зр╕Фр╕лр╕бр╕╣р╣Ир╕лр╕▒р╕зр╕Вр╣Йр╕н
    topic_categories = {
        "technology": CONTEXT_PATTERNS.get("tech", []) + CONTEXT_PATTERNS.get("gaming", []),
        "entertainment": CONTEXT_PATTERNS.get("music", []) + CONTEXT_PATTERNS.get("movie", []),
        "lifestyle": CONTEXT_PATTERNS.get("food", []) + CONTEXT_PATTERNS.get("travel", []),
        "business": CONTEXT_PATTERNS.get("business", []) + CONTEXT_PATTERNS.get("financial", [])
    }
    
    for category, words in topic_categories.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["topic_category"][category] = count
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕нр╕Зр╕Др╣Мр╕Ыр╕гр╕░р╕Бр╕нр╕Ър╕Чр╕▓р╕Зр╕зр╕▒р╕Тр╕Щр╕Шр╕гр╕гр╕б
    cultural_elements = {
        "traditional": CONTEXT_PATTERNS["traditional"],
        "religious": CONTEXT_PATTERNS["religious"],
        "regional": CONTEXT_PATTERNS["northern"] + CONTEXT_PATTERNS["southern"] + CONTEXT_PATTERNS["northeastern"]
    }
    
    for element, words in cultural_elements.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["cultural_elements"][element] = count
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Ар╕Др╕гр╕╖р╣Ир╕нр╕Зр╕лр╕бр╕▓р╕вр╕гр╕╕р╣Ир╕Щ
    generational_markers = {
        "gen_z": CONTEXT_PATTERNS["gen_z"],
        "millennial": CONTEXT_PATTERNS["millennial"],
        "gen_x": CONTEXT_PATTERNS["gen_x"]
    }
    
    for generation, words in generational_markers.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["generational_markers"][generation] = count
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕Хр╕▒р╕зр╕Ър╣Ир╕Зр╕Кр╕╡р╣Йр╕нр╕▓р╕гр╕бр╕Ур╣М
    emotional_indicators = {
        "positive_tone": CONTEXT_PATTERNS["praise"] + CONTEXT_PATTERNS["celebration"],
        "negative_tone": CONTEXT_PATTERNS["complaint"] + CONTEXT_PATTERNS["emergency"],
        "questioning": CONTEXT_PATTERNS["question"]
    }
    
    for indicator, words in emotional_indicators.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["emotional_indicators"][indicator] = count
    
    return patterns

def get_context_insights(context_data):
    """р╕кр╕гр╣Йр╕▓р╕З insights р╕Ир╕▓р╕Бр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ър╕гр╕┤р╕Ър╕Ч"""
    if not isinstance(context_data, dict):
        return {}
    
    insights = {
        "communication_appropriateness": "",
        "audience_recommendation": "",
        "tone_analysis": "",
        "cultural_sensitivity": "",
        "generational_appeal": ""
    }
    
    formality = context_data.get("formality_level", "neutral")
    social_setting = context_data.get("social_setting", "general")
    emotional_tone = context_data.get("emotional_tone", "neutral")
    generation = context_data.get("generation", "unknown")
    profession = context_data.get("profession", "general")
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕Др╕зр╕▓р╕бр╣Ар╕лр╕бр╕▓р╕░р╕кр╕бр╕Вр╕нр╕Зр╕Бр╕▓р╕гр╕кр╕╖р╣Ир╕нр╕кр╕▓р╕г
    if formality == "slang" and social_setting in ["business", "government", "education"]:
        insights["communication_appropriateness"] = "р╣Др╕бр╣Ир╣Ар╕лр╕бр╕▓р╕░р╕кр╕б: р╣Гр╕Кр╣Йр╕ар╕▓р╕йр╕▓р╕кр╣Бр╕ер╕Зр╣Гр╕Щр╕Ър╕гр╕┤р╕Ър╕Чр╕Чр╕▓р╕Зр╕Бр╕▓р╕г"
    elif formality == "formal" and social_setting == "social_media":
        insights["communication_appropriateness"] = "р╕нр╕▓р╕Ир╣Ар╕Ыр╣Зр╕Щр╕Чр╕▓р╕Зр╕Бр╕▓р╕гр╣Ар╕Бр╕┤р╕Щр╣Др╕Ы: р╕Др╕зр╕гр╕Ыр╕гр╕▒р╕Ър╣Ар╕Ыр╣Зр╕Щр╕ар╕▓р╕йр╕▓р╕Чр╕╡р╣Ир╣Ар╕Ыр╣Зр╕Щр╕Бр╕▒р╕Щр╣Ар╕нр╕Зр╕бр╕▓р╕Бр╕Вр╕╢р╣Йр╕Щ"
    else:
        insights["communication_appropriateness"] = "р╣Ар╕лр╕бр╕▓р╕░р╕кр╕б: р╕гр╕░р╕Фр╕▒р╕Ър╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╕Чр╕▓р╕Зр╕Бр╕▓р╕гр╕кр╕нр╕Фр╕Др╕ер╣Йр╕нр╕Зр╕Бр╕▒р╕Ър╕Ър╕гр╕┤р╕Ър╕Ч"
    
    # р╕Др╕│р╣Бр╕Щр╕░р╕Щр╕│р╕Бр╕ер╕╕р╣Ир╕бр╣Ар╕Ыр╣Йр╕▓р╕лр╕бр╕▓р╕в
    if generation != "unknown":
        insights["audience_recommendation"] = f"р╣Ар╕лр╕бр╕▓р╕░р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Бр╕ер╕╕р╣Ир╕б {generation}"
    else:
        insights["audience_recommendation"] = "р╣Ар╕лр╕бр╕▓р╕░р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Чр╕╕р╕Бр╕Бр╕ер╕╕р╣Ир╕бр╕нр╕▓р╕вр╕╕"
    
    # р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Вр╕Чр╕Щ
    if emotional_tone in ["complaint", "emergency"]:
        insights["tone_analysis"] = "р╣Вр╕Чр╕Щр╣Ар╕Кр╕┤р╕Зр╕ер╕Ъ: р╕Др╕зр╕гр╕гр╕░р╕зр╕▒р╕Зр╕Бр╕▓р╕гр╕Хр╕нр╕Ър╕кр╕Щр╕нр╕Зр╣Бр╕ер╕░р╣Бр╕Бр╣Йр╣Др╕Вр╕Ыр╕▒р╕Нр╕лр╕▓"
    elif emotional_tone in ["praise", "celebration"]:
        insights["tone_analysis"] = "р╣Вр╕Чр╕Щр╣Ар╕Кр╕┤р╕Зр╕Ър╕зр╕Б: р╣Вр╕нр╕Бр╕▓р╕кр╕кр╕гр╣Йр╕▓р╕Зр╕Др╕зр╕▓р╕бр╕кр╕▒р╕бр╕Юр╕▒р╕Щр╕Шр╣Мр╕Чр╕╡р╣Ир╕Фр╕╡"
    else:
        insights["tone_analysis"] = "р╣Вр╕Чр╕Щр╣Ар╕Ыр╣Зр╕Щр╕Бр╕ер╕▓р╕З: р╣Ар╕лр╕бр╕▓р╕░р╕кр╕│р╕лр╕гр╕▒р╕Ър╕Бр╕▓р╕гр╕кр╕╖р╣Ир╕нр╕кр╕▓р╕гр╕Чр╕▒р╣Ир╕зр╣Др╕Ы"
    
    # р╕Др╕зр╕▓р╕бр╣Др╕зр╕Чр╕▓р╕Зр╕зр╕▒р╕Тр╕Щр╕Шр╕гр╕гр╕б
    if profession in ["government", "education", "healthcare"]:
        insights["cultural_sensitivity"] = "р╕Хр╣Йр╕нр╕Зр╣Гр╕Кр╣Йр╕ар╕▓р╕йр╕▓р╕Чр╕╡р╣Ир╣Ар╕лр╕бр╕▓р╕░р╕кр╕бр╣Бр╕ер╕░р╣Др╕бр╣Ир╕ер╣Ир╕зр╕Зр╣Ар╕Бр╕┤р╕Щ"
    else:
        insights["cultural_sensitivity"] = "р╣Др╕бр╣Ир╕бр╕╡р╕Вр╣Йр╕нр╕Бр╕▒р╕Зр╕зр╕ер╕Чр╕▓р╕Зр╕зр╕▒р╕Тр╕Щр╕Шр╕гр╕гр╕бр╕Юр╕┤р╣Ар╕ир╕й"
    
    # р╕Др╕зр╕▓р╕бр╕Фр╕╢р╕Зр╕Фр╕╣р╕Фр╕Хр╕▓р╕бр╕гр╕╕р╣Ир╕Щ
    if generation == "gen_z":
        insights["generational_appeal"] = "р╣Гр╕Кр╣Йр╕ар╕▓р╕йр╕▓р╕Чр╕╡р╣Ир╕Чр╕▒р╕Щр╕кр╕бр╕▒р╕вр╣Бр╕ер╕░р╣Ар╕Вр╣Йр╕▓р╕Цр╕╢р╕Зр╕Др╕Щр╕гр╕╕р╣Ир╕Щр╣Гр╕лр╕бр╣И"
    elif generation == "millennial":
        insights["generational_appeal"] = "р╕ар╕▓р╕йр╕▓р╕Чр╕╡р╣Ир╕кр╕бр╕Фр╕╕р╕ер╕гр╕░р╕лр╕зр╣Ир╕▓р╕Зр╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╕Чр╕▓р╕Зр╕Бр╕▓р╕гр╣Бр╕ер╕░р╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╕Бр╕▒р╕Щр╣Ар╕нр╕З"
    elif generation == "gen_x":
        insights["generational_appeal"] = "р╕ар╕▓р╕йр╕▓р╕Чр╕╡р╣Ир╕бр╕▒р╣Ир╕Щр╕Др╕Зр╣Бр╕ер╕░р╣Ар╕Ыр╣Зр╕Щр╕гр╕░р╕Ър╕Ъ"
    else:
        insights["generational_appeal"] = "р╕ар╕▓р╕йр╕▓р╕Чр╕╡р╣Ир╣Ар╕Вр╣Йр╕▓р╣Гр╕Ир╣Др╕Фр╣Йр╕Чр╕╕р╕Бр╕Бр╕ер╕╕р╣Ир╕бр╕нр╕▓р╕вр╕╕"
    
    return insights
