import argparse
import os
import json
import subprocess
from glob import glob
from tqdm import tqdm
# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ml_sentiment_analysis ‡πÄ‡∏õ‡πá‡∏ô sentiment_integration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà
try:
    from sentiment_integration import analyze_detailed_sentiment, enhanced_analyze_sentiment
    DETAILED_SENTIMENT_AVAILABLE = True
except ImportError:
    # Fallback ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡∏î‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°
    try:
        from ml_sentiment_analysis import analyze_sentiment
        DETAILED_SENTIMENT_AVAILABLE = False
    except ImportError:
        def analyze_sentiment(text):
            return {"sentiment": "neutral", "confidence": 0.0, "sentiment_score": 0.0}
        DETAILED_SENTIMENT_AVAILABLE = False

import time
import random
import hashlib
import re

# === EMOTION LABEL SCHEMA ===
EMOTION_LABELS = [
    # Positive
    "‡∏î‡∏µ‡πÉ‡∏à", "‡∏ä‡∏≠‡∏ö", "‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏à", "‡∏û‡∏≠‡πÉ‡∏à", "‡∏£‡∏±‡∏Å",
    
    # Negative  
    "‡πÇ‡∏Å‡∏£‡∏ò", "‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á", "‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç", "‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î", "‡∏Å‡∏•‡∏±‡∏ß", "‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î", "‡∏ï‡∏Å‡πÉ‡∏à",
    
    # Neutral
    "‡πÄ‡∏â‡∏¢ ‡πÜ", "‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏∞‡πÑ‡∏£", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£",
    
    # Others (Complex emotions)
    "‡∏õ‡∏£‡∏∞‡∏ä‡∏î", "‡∏Ç‡∏≥‡∏Ç‡∏±‡∏ô", "‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ", "‡∏™‡∏±‡∏ö‡∏™‡∏ô"
]

# Emotion grouping
EMOTION_GROUPS = {
    "Positive": ["‡∏î‡∏µ‡πÉ‡∏à", "‡∏ä‡∏≠‡∏ö", "‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏à", "‡∏û‡∏≠‡πÉ‡∏à", "‡∏£‡∏±‡∏Å"],
    "Negative": ["‡πÇ‡∏Å‡∏£‡∏ò", "‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á", "‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç", "‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î", "‡∏Å‡∏•‡∏±‡∏ß", "‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î", "‡∏ï‡∏Å‡πÉ‡∏à"],
    "Neutral": ["‡πÄ‡∏â‡∏¢ ‡πÜ", "‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏∞‡πÑ‡∏£", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£"],
    "Others": ["‡∏õ‡∏£‡∏∞‡∏ä‡∏î", "‡∏Ç‡∏≥‡∏Ç‡∏±‡∏ô", "‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ", "‡∏™‡∏±‡∏ö‡∏™‡∏ô"]
}

# Reverse mapping for quick lookup
LABEL_TO_GROUP = {}
for group, labels in EMOTION_GROUPS.items():
    for label in labels:
        LABEL_TO_GROUP[label] = group

# === EMOTION PATTERNS ===
EMOTION_PATTERNS = {
    # === POSITIVE EMOTIONS ===
    "‡∏î‡∏µ‡πÉ‡∏à": {
        "keywords": ["‡∏î‡∏µ‡πÉ‡∏à", "‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç", "‡πÅ‡∏Æ‡∏õ‡∏õ‡∏µ‡πâ", "‡∏õ‡∏•‡∏∑‡πâ‡∏°", "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ", "‡πÄ‡∏Æ‡∏á", "‡πÄ‡∏¢‡πâ", "‡πÇ‡∏¢‡πà", "‡πÄ‡∏à‡πã‡∏á", "‡∏î‡∏µ‡πà‡πÉ‡∏à"],
        "patterns": [r"‡∏î‡∏µ\s*‡πÉ‡∏à", r"‡∏õ‡∏•‡∏∑‡πâ‡∏°", r"‡πÄ‡∏Æ‡∏á\s*‡∏ã‡∏∞", r"‡πÅ‡∏Æ‡∏õ‡∏õ‡∏µ‡πâ", r"‡πÄ‡∏¢‡πâ.*", r"‡πÇ‡∏¢‡πà.*"],
        "emojis": ["üòä", "üòÑ", "ü§ó", "üòç", "ü•∞", "üòò", "üòÜ", "ü§©"],
        "score_range": (0.6, 1.0)
    },
    
    "‡∏ä‡∏≠‡∏ö": {
        "keywords": ["‡∏ä‡∏≠‡∏ö", "‡∏£‡∏±‡∏Å", "‡∏ñ‡∏π‡∏Å‡πÉ‡∏à", "‡πÇ‡∏õ‡∏£‡∏î", "‡∏õ‡∏•‡∏∑‡πâ‡∏°", "‡∏™‡∏ô‡πÉ‡∏à", "‡∏≠‡∏¥‡∏ô", "‡πÄ‡∏Ñ‡∏•‡∏¥‡πâ‡∏°"],
        "patterns": [r"‡∏ä‡∏≠‡∏ö.*‡∏°‡∏≤‡∏Å", r"‡∏£‡∏±‡∏Å.*‡πÄ‡∏•‡∏¢", r"‡∏ñ‡∏π‡∏Å‡πÉ‡∏à", r"‡πÇ‡∏õ‡∏£‡∏î.*", r"‡∏™‡∏ô‡πÉ‡∏à.*‡∏°‡∏≤‡∏Å"],
        "emojis": ["‚ù§Ô∏è", "üíï", "üòç", "ü•∞", "üòò", "üíñ", "üíù"],
        "score_range": (0.5, 0.9)
    },
    
    "‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏à": {
        "keywords": ["‡∏ã‡∏∂‡πâ‡∏á", "‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏à", "‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏ã‡∏∂‡∏°", "‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à", "‡∏ã‡∏≤‡∏ö‡∏ã‡∏∂‡πâ‡∏á", "‡∏ï‡∏∑‡πâ‡∏ô‡∏ï‡∏±‡∏ô", "‡∏ã‡∏∑‡πà‡∏ô‡πÉ‡∏™"],
        "patterns": [r"‡∏ã‡∏∂‡πâ‡∏á.*‡πÉ‡∏à", r"‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à", r"‡∏ô‡πâ‡∏≥‡∏ï‡∏≤.*‡∏ã‡∏∂‡∏°", r"‡∏ã‡∏≤‡∏ö‡∏ã‡∏∂‡πâ‡∏á"],
        "emojis": ["üò≠", "ü•∫", "üò¢", "ü§ß", "üíû"],
        "score_range": (0.4, 0.8)
    },
    
    "‡∏û‡∏≠‡πÉ‡∏à": {
        "keywords": ["‡∏û‡∏≠‡πÉ‡∏à", "‡πÇ‡∏≠‡πÄ‡∏Ñ", "‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ", "‡∏õ‡∏Å‡∏ï‡∏¥‡∏î‡∏µ", "‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£", "‡∏á‡∏≤‡∏°", "‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢"],
        "patterns": [r"‡∏û‡∏≠‡πÉ‡∏à", r"‡πÇ‡∏≠‡πÄ‡∏Ñ.*", r"‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ", r"‡∏õ‡∏Å‡∏ï‡∏¥‡∏î‡∏µ", r"‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢"],
        "emojis": ["üëç", "üëå", "üòå", "üôÇ"],
        "score_range": (0.2, 0.6)
    },
    
    "‡∏£‡∏±‡∏Å": {
        "keywords": ["‡∏£‡∏±‡∏Å", "‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å", "‡πÅ‡∏û‡∏á", "‡πÄ‡∏•‡∏¥‡∏ü", "love", "‡∏Æ‡∏±‡∏Å‡πÜ", "‡∏Æ‡∏±‡∏Å", "‡∏£‡∏±‡∏Å‡∏°‡∏≤‡∏Å"],
        "patterns": [r"‡∏£‡∏±‡∏Å.*‡∏°‡∏≤‡∏Å", r"‡∏´‡∏•‡∏á‡∏£‡∏±‡∏Å", r"‡πÄ‡∏•‡∏¥‡∏ü.*", r"love.*", r"‡∏Æ‡∏±‡∏Å.*"],
        "emojis": ["‚ù§Ô∏è", "üíï", "üíñ", "üíù", "üòç", "ü•∞", "üòò"],
        "score_range": (0.7, 1.0)
    },
    
    # === NEGATIVE EMOTIONS ===
    "‡πÇ‡∏Å‡∏£‡∏ò": {
        "keywords": ["‡πÇ‡∏Å‡∏£‡∏ò", "‡∏â‡∏∏‡∏ô", "‡πÇ‡∏°‡πÇ‡∏´", "‡πÅ‡∏Ñ‡πâ‡∏ô", "‡∏Ç‡∏∏‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡∏á", "‡πÄ‡∏î‡∏∑‡∏≠‡∏î", "‡∏ö‡πâ‡∏≤", "‡∏´‡πà‡∏ß‡∏¢‡πÅ‡∏ï‡∏Å", "‡πÅ‡∏¢‡πà", "‡∏á‡∏µ‡πà‡πÄ‡∏á‡πà‡∏≤"],
        "patterns": [r"‡πÇ‡∏Å‡∏£‡∏ò.*‡∏°‡∏≤‡∏Å", r"‡∏â‡∏∏‡∏ô.*‡∏Ç‡∏≤‡∏î", r"‡πÇ‡∏°‡πÇ‡∏´", r"‡πÅ‡∏Ñ‡πâ‡∏ô.*", r"‡∏´‡πà‡∏ß‡∏¢.*‡πÅ‡∏ï‡∏Å", r"‡∏ö‡πâ‡∏≤.*", r"‡πÅ‡∏¢‡πà.*‡∏°‡∏≤‡∏Å"],
        "emojis": ["üò†", "üò°", "ü§¨", "üëø", "üí¢", "üò§"],
        "score_range": (-1.0, -0.6)
    },
    
    "‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à": {
        "keywords": ["‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡πÄ‡∏®‡∏£‡πâ‡∏≤", "‡πÉ‡∏à‡∏´‡∏≤‡∏¢", "‡∏õ‡∏ß‡∏î‡πÉ‡∏à", "‡πÄ‡∏®‡∏£‡πâ‡∏≤‡πÇ‡∏®‡∏Å", "‡πÇ‡∏®‡∏Å‡πÄ‡∏®‡∏£‡πâ‡∏≤", "‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏≤‡∏¢", "‡∏ô‡πà‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à"],
        "patterns": [r"‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", r"‡πÄ‡∏®‡∏£‡πâ‡∏≤.*‡∏°‡∏≤‡∏Å", r"‡πÉ‡∏à‡∏´‡∏≤‡∏¢", r"‡∏õ‡∏ß‡∏î‡πÉ‡∏à", r"‡πÄ‡∏®‡∏£‡πâ‡∏≤‡πÇ‡∏®‡∏Å"],
        "emojis": ["üò¢", "üò≠", "üòû", "‚òπÔ∏è", "üòî", "üíî"],
        "score_range": (-0.8, -0.4)
    },
    
    "‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á": {
        "keywords": ["‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á", "‡∏´‡∏ß‡∏±‡∏á‡πÄ‡∏Å‡∏¥‡∏ô", "‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á", "‡∏ó‡πâ‡∏≠", "‡∏´‡∏°‡∏î‡∏´‡∏ß‡∏±‡∏á", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏î‡∏±‡∏á‡πÉ‡∏à"],
        "patterns": [r"‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á", r"‡∏´‡∏ß‡∏±‡∏á.*‡πÄ‡∏Å‡∏¥‡∏ô", r"‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á.*‡∏°‡∏≤‡∏Å", r"‡∏ó‡πâ‡∏≠.*", r"‡∏´‡∏°‡∏î‡∏´‡∏ß‡∏±‡∏á"],
        "emojis": ["üòû", "üòî", "üòì", "üò©", "üò§"],
        "score_range": (-0.7, -0.3)
    },
    
    "‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç": {
        "keywords": ["‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç", "‡∏ô‡πà‡∏≤‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç", "‡πÄ‡∏ö‡∏∑‡πà‡∏≠", "‡∏´‡∏ô‡πà‡∏≤‡∏¢", "‡πÄ‡∏ã‡πá‡∏á", "‡∏á‡πà‡∏ß‡∏á", "‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î", "‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢"],
        "patterns": [r"‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç", r"‡πÄ‡∏ö‡∏∑‡πà‡∏≠.*‡∏°‡∏≤‡∏Å", r"‡∏´‡∏ô‡πà‡∏≤‡∏¢.*", r"‡πÄ‡∏ã‡πá‡∏á.*", r"‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î.*"],
        "emojis": ["üòí", "üôÑ", "üò§", "üòë", "üò´", "üò©"],
        "score_range": (-0.6, -0.2)
    },
    
    "‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î": {
        "keywords": ["‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î", "‡∏Ç‡∏¢‡∏∞‡πÅ‡∏Ç‡∏¢‡∏á", "‡πÅ‡∏Ñ‡πâ‡∏ô", "‡πÅ‡∏Å‡∏•‡πâ‡∏á", "‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö", "‡∏ï‡πà‡∏≠‡∏ï‡πâ‡∏≤‡∏ô"],
        "patterns": [r"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î.*‡∏°‡∏≤‡∏Å", r"‡∏Ç‡∏¢‡∏∞‡πÅ‡∏Ç‡∏¢‡∏á", r"‡πÅ‡∏Ñ‡πâ‡∏ô.*", r"‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö.*‡πÄ‡∏•‡∏¢"],
        "emojis": ["üò°", "ü§¨", "üëø", "üò†", "üí¢"],
        "score_range": (-1.0, -0.7)
    },
    
    "‡∏Å‡∏•‡∏±‡∏ß": {
        "keywords": ["‡∏Å‡∏•‡∏±‡∏ß", "‡∏´‡∏ß‡∏≤‡∏î‡∏Å‡∏•‡∏±‡∏ß", "‡∏ï‡∏Å‡πÉ‡∏à", "‡∏ß‡∏¥‡∏ï‡∏Å", "‡∏Å‡∏±‡∏á‡∏ß‡∏•", "‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î", "‡∏´‡∏ß‡∏±‡πà‡∏ô", "‡∏ï‡∏∑‡πà‡∏ô‡∏Å‡∏•‡∏±‡∏ß"],
        "patterns": [r"‡∏Å‡∏•‡∏±‡∏ß.*‡∏°‡∏≤‡∏Å", r"‡∏´‡∏ß‡∏≤‡∏î‡∏Å‡∏•‡∏±‡∏ß", r"‡∏ï‡∏Å‡πÉ‡∏à.*", r"‡∏ß‡∏¥‡∏ï‡∏Å.*", r"‡∏Å‡∏±‡∏á‡∏ß‡∏•.*"],
        "emojis": ["üò®", "üò∞", "üò±", "üòß", "ü´£", "üò≥"],
        "score_range": (-0.8, -0.3)
    },
    
    "‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î": {
        "keywords": ["‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î", "‡∏≠‡∏±‡∏ö‡∏≠‡∏≤‡∏¢", "‡πÄ‡∏Å‡πâ‡∏≠", "‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡πÉ‡∏à", "‡∏Å‡∏î‡∏î‡∏±‡∏ô", "‡∏Ç‡∏±‡∏î‡πÉ‡∏à"],
        "patterns": [r"‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î", r"‡∏≠‡∏±‡∏ö‡∏≠‡∏≤‡∏¢", r"‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡πÉ‡∏à", r"‡∏Å‡∏î‡∏î‡∏±‡∏ô", r"‡∏Ç‡∏±‡∏î‡πÉ‡∏à"],
        "emojis": ["üò£", "üòñ", "üò´", "üò§", "üò∞"],
        "score_range": (-0.6, -0.2)
    },
    
    "‡∏ï‡∏Å‡πÉ‡∏à": {
        "keywords": ["‡∏ï‡∏Å‡πÉ‡∏à", "‡∏™‡∏∞‡∏î‡∏∏‡πâ‡∏á", "‡πÇ‡∏´‡∏¢‡∏á", "‡∏ï‡∏Å‡∏ï‡∏∞‡∏•‡∏∂‡∏á", "‡∏ï‡∏∞‡∏•‡∏∂‡∏á", "‡∏´‡∏ß‡∏≤‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏ß"],
        "patterns": [r"‡∏ï‡∏Å‡πÉ‡∏à.*‡∏°‡∏≤‡∏Å", r"‡∏™‡∏∞‡∏î‡∏∏‡πâ‡∏á", r"‡πÇ‡∏´‡∏¢‡∏á", r"‡∏ï‡∏Å‡∏ï‡∏∞‡∏•‡∏∂‡∏á", r"‡∏ï‡∏∞‡∏•‡∏∂‡∏á"],
        "emojis": ["üò±", "üò®", "üò≥", "ü´®", "üòß"],
        "score_range": (-0.5, 0.0)
    },
    
    # === NEUTRAL EMOTIONS ===
    "‡πÄ‡∏â‡∏¢ ‡πÜ": {
        "keywords": ["‡πÄ‡∏â‡∏¢", "‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤", "‡∏õ‡∏Å‡∏ï‡∏¥", "‡πÇ‡∏≠‡πÄ‡∏Ñ", "‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ", "‡∏û‡∏≠‡πÉ‡∏ä‡πâ", "‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£"],
        "patterns": [r"‡πÄ‡∏â‡∏¢.*‡πÜ", r"‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤", r"‡∏õ‡∏Å‡∏ï‡∏¥.*", r"‡πÇ‡∏≠‡πÄ‡∏Ñ", r"‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£"],
        "emojis": ["üòê", "üôÇ", "üò∂", "üòë"],
        "score_range": (-0.1, 0.1)
    },
    
    "‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏∞‡πÑ‡∏£": {
        "keywords": ["‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å", "‡∏ä‡∏≤", "‡πÄ‡∏â‡∏¢", "‡πÑ‡∏°‡πà‡∏™‡∏ô", "‡πÑ‡∏°‡πà‡πÅ‡∏Ñ‡∏£‡πå", "‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à"],
        "patterns": [r"‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å.*‡∏≠‡∏∞‡πÑ‡∏£", r"‡∏ä‡∏≤.*", r"‡πÑ‡∏°‡πà‡∏™‡∏ô.*", r"‡πÑ‡∏°‡πà‡πÅ‡∏Ñ‡∏£‡πå"],
        "emojis": ["üò∂", "üòê", "ü§∑‚Äç‚ôÄÔ∏è", "ü§∑‚Äç‚ôÇÔ∏è"],
        "score_range": (-0.05, 0.05)
    },
    
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£": {
        "keywords": ["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏Ç‡πà‡∏≤‡∏ß", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡πÅ‡∏à‡πâ‡∏á", "‡∏ö‡∏≠‡∏Å", "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï", "‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"],
        "patterns": [r"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•.*", r"‡∏Ç‡πà‡∏≤‡∏ß.*", r"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô.*", r"‡πÅ‡∏à‡πâ‡∏á.*", r"‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï.*"],
        "emojis": ["üì∞", "üìä", "üìà", "üì¢", "‚ÑπÔ∏è"],
        "score_range": (0.0, 0.0)
    },
    
    # === OTHERS (COMPLEX EMOTIONS) ===
    "‡∏õ‡∏£‡∏∞‡∏ä‡∏î": {
        "keywords": ["‡∏õ‡∏£‡∏∞‡∏ä‡∏î", "‡πÄ‡∏´‡∏ô‡πá‡∏ö‡πÅ‡∏ô‡∏°", "‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ", "‡πÅ‡∏î‡∏Å‡∏î‡∏±‡∏ô", "‡∏à‡∏¥‡∏Å‡∏Å‡∏±‡∏î", "‡∏≠‡∏µ‡∏î‡∏≠‡∏Å"],
        "patterns": [r"‡∏õ‡∏£‡∏∞‡∏ä‡∏î.*", r"‡πÄ‡∏´‡∏ô‡πá‡∏ö‡πÅ‡∏ô‡∏°", r"‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ.*", r"‡πÅ‡∏î‡∏Å‡∏î‡∏±‡∏ô", r"‡∏à‡∏¥‡∏Å‡∏Å‡∏±‡∏î"],
        "emojis": ["üòè", "üôÑ", "üòí", "üò§"],
        "score_range": (-0.4, -0.1)
    },
    
    "‡∏Ç‡∏≥‡∏Ç‡∏±‡∏ô": {
        "keywords": ["‡∏Ç‡∏≥", "‡∏ï‡∏•‡∏Å", "555", "‡∏Æ‡∏≤", "‡πÄ‡∏Æ‡∏Æ‡∏≤", "‡∏™‡∏ô‡∏∏‡∏Å", "‡πÇ‡∏•‡∏Å‡πÅ‡∏ï‡∏Å", "‡∏Ñ‡∏£‡∏∑‡πà‡∏ô‡πÄ‡∏Ñ‡∏£‡∏á"],
        "patterns": [r"‡∏Ç‡∏≥.*", r"‡∏ï‡∏•‡∏Å.*", r"555+", r"‡∏Æ‡∏≤+", r"‡πÄ‡∏Æ‡∏Æ‡∏≤", r"‡∏™‡∏ô‡∏∏‡∏Å.*"],
        "emojis": ["üòÇ", "ü§£", "üòÜ", "üòÑ", "üòÅ", "ü§™", "üòú"],
        "score_range": (0.3, 0.8)
    },
    
    "‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ": {
        "keywords": ["‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ", "‡∏õ‡∏£‡∏∞‡∏ä‡∏î", "‡πÄ‡∏´‡∏ô‡πá‡∏ö", "‡πÅ‡∏ô‡∏°", "‡∏à‡∏¥‡∏Å‡∏Å‡∏±‡∏î", "‡πÅ‡∏Å‡∏•‡πâ‡∏á"],
        "patterns": [r"‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏™‡∏µ.*", r"‡∏õ‡∏£‡∏∞‡∏ä‡∏î.*", r"‡πÄ‡∏´‡∏ô‡πá‡∏ö.*‡πÅ‡∏ô‡∏°", r"‡∏à‡∏¥‡∏Å‡∏Å‡∏±‡∏î.*"],
        "emojis": ["üòè", "üôÑ", "üòí"],
        "score_range": (-0.5, -0.2)
    },
    
    "‡∏™‡∏±‡∏ö‡∏™‡∏ô": {
        "keywords": ["‡∏™‡∏±‡∏ö‡∏™‡∏ô", "‡∏á‡∏á", "‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ", "‡πÅ‡∏õ‡∏•‡∏Å", "‡∏â‡∏á‡∏ô", "‡∏â‡∏á‡∏ô‡∏™‡∏ô‡πÄ‡∏ó‡πà‡∏´‡πå"],
        "patterns": [r"‡∏™‡∏±‡∏ö‡∏™‡∏ô", r"‡∏á‡∏á.*", r"‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ", r"‡πÅ‡∏õ‡∏•‡∏Å.*", r"‡∏â‡∏á‡∏ô.*"],
        "emojis": ["üòï", "ü§î", "üòµ‚Äçüí´", "ü´§", "üòµ"],
        "score_range": (-0.2, 0.2)
    }
}

# === COMPREHENSIVE CONTEXT PATTERNS ===
CONTEXT_PATTERNS = {
    # === ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ===
    "formal": ["‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏∞", "‡∏Ç‡∏≠", "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏ó‡πà‡∏≤‡∏ô", "‡∏Ñ‡∏∏‡∏ì", "‡∏û‡∏µ‡πà", "‡∏ô‡πâ‡∏≠‡∏á", "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏≤‡∏£‡∏û"],
    "informal": ["‡∏ô‡∏∞", "‡πÄ‡∏ô‡∏≠‡∏∞", "‡∏≠‡∏∞", "‡πÄ‡∏≠‡πâ‡∏¢", "‡πÄ‡∏≠‡∏≠", "‡∏Ç‡∏≠‡∏á", "555", "‡∏Æ‡∏≤", "‡∏à‡πâ‡∏∞", "‡∏à‡πã‡∏≤", "‡∏ß‡πà‡∏∞", "‡∏ß‡∏∞", "‡πÄ‡∏Æ‡πâ‡∏¢"],
    "slang": ["‡πÇ‡∏Ñ‡∏ï‡∏£", "‡πÄ‡∏ü‡∏µ‡πâ‡∏¢‡∏ß", "‡πÄ‡∏ó‡∏û", "‡πÅ‡∏°‡πà‡∏á", "‡∏Ñ‡∏ß‡∏¢", "‡∏ö‡∏¥‡∏ô", "‡πÄ‡∏ü‡∏µ‡πâ‡∏¢‡∏°", "‡∏ä‡∏¥‡∏ö", "‡πÄ‡∏î‡πá‡∏î", "‡∏õ‡∏±‡∏á", "‡∏´‡πà‡∏ß‡∏¢", "‡∏ã‡∏ß‡∏¢"],
    
    # === ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß ===
    "personal": ["‡∏Å‡∏π", "‡∏°‡∏∂‡∏á", "‡πÄ‡∏£‡∏≤", "‡∏â‡∏±‡∏ô", "‡∏Ñ‡∏¥‡∏î", "‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å", "‡πÉ‡∏à", "‡∏´‡∏±‡∏ß‡πÉ‡∏à", "‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á", "‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß"],
    "intimate": ["‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å", "‡∏´‡∏ß‡∏≤‡∏ô‡πÉ‡∏à", "‡∏î‡∏≤‡∏£‡πå‡∏•‡∏¥‡πà‡∏á", "‡∏Æ‡∏±‡∏ô‡∏ô‡∏µ‡πà", "‡πÄ‡∏ö‡∏ö‡∏µ‡πâ", "‡∏Ñ‡∏ô‡∏î‡∏µ", "‡πÄ‡∏™‡∏∑‡∏≠", "‡∏´‡∏ô‡∏π‡πÄ‡∏≠‡∏á"],
    "friendly": ["‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô", "‡πÄ‡∏ü‡∏£‡πâ‡∏ô", "‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤", "‡πÅ‡∏Å‡πä‡∏á", "‡∏Å‡∏•‡∏∏‡πà‡∏°", "‡∏Ñ‡∏ô‡πÄ‡∏Å‡πà‡∏≤", "‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏° ===
    "social_media": ["‡πÅ‡∏ä‡∏£‡πå", "‡πÑ‡∏•‡∏Ñ‡πå", "‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå", "‡πÇ‡∏û‡∏™‡∏ï‡πå", "‡πÅ‡∏ó‡πá‡∏Å", "‡πÄ‡∏ü‡∏™‡∏ö‡∏∏‡πä‡∏Ñ", "‡πÑ‡∏≠‡∏à‡∏µ", "‡∏ó‡∏ß‡∏¥‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏ï‡∏¥‡πä‡∏Å‡∏ï‡πä‡∏≠‡∏Å", "‡∏¢‡∏π‡∏ó‡∏π‡∏õ"],
    "news_media": ["‡∏Ç‡πà‡∏≤‡∏ß", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡πÅ‡∏à‡πâ‡∏á‡∏Ç‡πà‡∏≤‡∏ß", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡πÅ‡∏ñ‡∏•‡∏á‡∏Å‡∏≤‡∏£‡∏ì‡πå", "‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç", "‡∏î‡πà‡∏ß‡∏ô"],
    "review": ["‡∏£‡∏µ‡∏ß‡∏¥‡∏ß", "‡∏ó‡∏î‡∏•‡∏≠‡∏á", "‡πÉ‡∏ä‡πâ‡∏î‡∏π", "‡∏•‡∏≠‡∏á", "‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå", "‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û", "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "‡∏£‡πâ‡∏≤‡∏ô"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ===
    "complaint": ["‡∏ö‡πà‡∏ô", "‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡πÅ‡∏à‡πâ‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ", "‡πÄ‡∏™‡∏µ‡∏¢", "‡∏´‡πà‡∏ß‡∏¢", "‡πÅ‡∏¢‡πà", "‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", "‡∏ä‡πâ‡∏≤"],
    "praise": ["‡∏ä‡∏°", "‡∏¢‡∏Å‡∏¢‡πà‡∏≠‡∏á", "‡∏î‡∏µ", "‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°", "‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à", "‡∏ä‡∏≠‡∏ö", "‡∏£‡∏±‡∏Å", "‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î", "‡πÄ‡∏à‡πã‡∏á", "‡πÄ‡∏ó‡∏û"],
    "question": ["‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡∏∞", "‡∏°‡∏±‡πâ‡∏¢", "‡∏´‡∏£‡∏∑‡∏≠", "‡πÑ‡∏´‡∏°", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡∏¢‡∏±‡∏á‡πÑ‡∏á", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå ===
    "emergency": ["‡∏î‡πà‡∏ß‡∏ô", "‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô", "‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡πÄ‡∏™‡∏µ‡∏¢", "‡∏û‡∏±‡∏á", "‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢", "‡∏ß‡∏¥‡∏Å‡∏§‡∏ï"],
    "celebration": ["‡∏¢‡∏¥‡∏ô‡∏î‡∏µ", "‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏¥‡∏ô‡∏î‡∏µ", "‡∏Ç‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏¥‡∏ô‡∏î‡∏µ", "‡∏î‡∏µ‡πÉ‡∏à", "‡∏õ‡∏•‡∏∑‡πâ‡∏°‡∏õ‡∏µ‡∏ï‡∏¥", "‡πÄ‡∏Æ‡∏á", "‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ"],
    "condolence": ["‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡∏Ç‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à", "‡πÄ‡∏®‡∏£‡πâ‡∏≤", "‡∏≠‡∏≤‡∏•‡∏±‡∏¢", "‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏° ===
    "religious": ["‡∏ö‡∏∏‡∏ç", "‡∏Å‡∏£‡∏£‡∏°", "‡∏ò‡∏£‡∏£‡∏°", "‡∏û‡∏£‡∏∞", "‡∏ß‡∏±‡∏î", "‡∏ô‡∏°‡∏±‡∏™‡∏Å‡∏≤‡∏£", "‡πÑ‡∏´‡∏ß‡πâ", "‡∏®‡∏≤‡∏™‡∏ô‡∏≤", "‡∏ö‡∏≤‡∏õ", "‡∏Å‡∏∏‡∏®‡∏•"],
    "traditional": ["‡∏õ‡∏£‡∏∞‡πÄ‡∏û‡∏ì‡∏µ", "‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°", "‡πÑ‡∏ó‡∏¢", "‡πÇ‡∏ö‡∏£‡∏≤‡∏ì", "‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°", "‡∏†‡∏π‡∏°‡∏¥‡∏õ‡∏±‡∏ç‡∏ç‡∏≤", "‡∏ä‡∏≤‡∏ß‡∏ö‡πâ‡∏≤‡∏ô"],
    "modern": ["‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢", "‡πÇ‡∏°‡πÄ‡∏î‡∏¥‡∏£‡πå‡∏ô", "‡πÑ‡∏Æ‡πÄ‡∏ó‡∏Ñ", "‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•", "‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå", "‡πÅ‡∏≠‡∏õ", "‡πÑ‡∏≠‡∏ó‡∏µ", "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ===
    "central": ["‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û", "‡∏Å‡∏ó‡∏°", "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á", "‡∏†‡∏≤‡∏Ñ‡∏Å‡∏•‡∏≤‡∏á", "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á"],
    "northern": ["‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà", "‡∏†‡∏≤‡∏Ñ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠", "‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤", "‡∏Ñ‡∏≥‡πÄ‡∏°‡∏∑‡∏≠‡∏á", "‡∏ô‡∏≤", "‡∏õ‡πà‡∏≤"],
    "southern": ["‡πÉ‡∏ï‡πâ", "‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ", "‡∏ó‡∏∞‡πÄ‡∏•", "‡∏õ‡∏•‡∏≤", "‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤", "‡∏õ‡∏≤‡∏•‡πå‡∏°"],
    "northeastern": ["‡∏≠‡∏µ‡∏™‡∏≤‡∏ô", "‡∏†‡∏≤‡∏Ñ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô", "‡∏™‡πâ‡∏°‡∏ï‡∏≥", "‡∏•‡∏≤‡∏ß", "‡∏Ç‡πâ‡∏≤‡∏ß‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß", "‡πÅ‡∏à‡πà‡∏ß"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏≤‡∏¢‡∏∏/‡∏£‡∏∏‡πà‡∏ô ===
    "gen_z": ["‡∏õ‡∏±‡∏á", "‡πÄ‡∏î‡πá‡∏î", "‡∏ü‡∏¥‡∏ô", "‡∏ä‡∏¥‡∏•", "‡πÄ‡∏ü‡∏•‡πá‡∏Å‡∏ã‡πå", "‡πÇ‡∏ö", "‡∏•‡∏¥‡∏ï", "‡πÑ‡∏ß‡∏ö‡πå", "‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ó‡∏ô‡∏ï‡πå"],
    "millennial": ["‡πÇ‡∏≠‡πÄ‡∏Ñ", "‡πÄ‡∏ü‡∏™", "‡πÑ‡∏•‡∏ô‡πå", "‡∏≠‡∏¥‡∏ô‡∏™‡∏ï‡∏≤", "‡∏ã‡∏µ‡∏£‡∏µ‡πà‡∏¢‡πå", "‡∏¢‡∏π‡∏ó‡∏π‡∏õ", "‡∏Å‡∏π‡πÄ‡∏Å‡∏¥‡∏•"],
    "gen_x": ["‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤", "‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠", "‡∏™‡∏°‡∏±‡∏¢‡∏Å‡πà‡∏≠‡∏ô", "‡∏ï‡∏≠‡∏ô‡∏´‡∏ô‡∏∏‡πà‡∏°", "‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û ===
    "business": ["‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à", "‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î", "‡∏Ç‡∏≤‡∏¢", "‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤", "‡∏Å‡∏≥‡πÑ‡∏£", "‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô", "‡∏•‡∏á‡∏ó‡∏∏‡∏ô"],
    "education": ["‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏™‡∏≠‡∏ô", "‡∏Ñ‡∏£‡∏π", "‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "‡∏ß‡∏¥‡∏ä‡∏≤"],
    "healthcare": ["‡∏´‡∏°‡∏≠", "‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å", "‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•", "‡∏¢‡∏≤", "‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û", "‡∏õ‡πà‡∏ß‡∏¢"],
    "government": ["‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£", "‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö", "‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ ===
    "gaming": ["‡πÄ‡∏Å‡∏°", "‡πÄ‡∏•‡πà‡∏ô", "‡πÅ‡∏£‡∏á‡∏Ñ‡πå", "‡∏™‡∏Å‡∏¥‡∏•", "‡∏≠‡∏±‡∏û", "‡πÄ‡∏•‡πÄ‡∏ß‡∏•", "PvP", "‡πÑ‡∏≠‡πÄ‡∏ó‡∏°"],
    "tech": ["‡πÇ‡∏Ñ‡πâ‡∏î", "‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°", "‡πÅ‡∏≠‡∏õ", "‡πÄ‡∏ß‡πá‡∏ö", "AI", "‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠", "‡∏Ñ‡∏≠‡∏°", "‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå"],
    "crypto": ["‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï", "‡∏ö‡∏¥‡∏ó‡∏Ñ‡∏≠‡∏¢‡∏ô‡πå", "‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç", "‡πÄ‡∏ó‡∏£‡∏î", "‡∏Ç‡∏∏‡∏î", "‡∏ß‡∏≠‡∏•‡πÄ‡∏•‡πá‡∏ï", "NFT"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡πÄ‡∏ó‡∏¥‡∏á ===
    "music": ["‡πÄ‡∏û‡∏•‡∏á", "‡∏®‡∏¥‡∏•‡∏õ‡∏¥‡∏ô", "‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ï", "‡∏≠‡∏±‡∏•‡∏ö‡∏±‡πâ‡∏°", "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏û‡∏•‡∏á", "‡∏î‡∏ô‡∏ï‡∏£‡∏µ", "‡πÅ‡∏£‡πá‡∏õ"],
    "movie": ["‡∏´‡∏ô‡∏±‡∏á", "‡∏ã‡∏µ‡∏£‡∏µ‡πà‡∏¢‡πå", "‡∏ô‡∏±‡∏Å‡πÅ‡∏™‡∏î‡∏á", "‡∏ú‡∏π‡πâ‡∏Å‡∏≥‡∏Å‡∏±‡∏ö", "‡πÇ‡∏£‡∏á‡∏´‡∏ô‡∏±‡∏á", "Netflix", "‡∏î‡∏π‡∏´‡∏ô‡∏±‡∏á"],
    "sports": ["‡∏Å‡∏µ‡∏¨‡∏≤", "‡∏ó‡∏µ‡∏°", "‡πÅ‡∏Ç‡πà‡∏á", "‡∏ä‡∏ô‡∏∞", "‡πÅ‡∏û‡πâ", "‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•", "‡∏ö‡∏≤‡∏™", "‡∏ß‡∏¥‡πà‡∏á"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å ===
    "romance": ["‡∏Ñ‡∏π‡πà‡∏£‡∏±‡∏Å", "‡πÅ‡∏ü‡∏ô", "‡∏´‡∏ß‡∏≤‡∏ô", "‡πÇ‡∏£‡πÅ‡∏°‡∏ô‡∏ï‡∏¥‡∏Å", "‡∏à‡∏µ‡∏ö", "‡πÄ‡∏î‡∏ó", "‡∏Ñ‡∏ö", "‡∏£‡∏±‡∏Å"],
    "breakup": ["‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏±‡∏ô", "‡∏ó‡∏¥‡πâ‡∏á", "‡∏´‡∏•‡∏≠‡∏Å", "‡∏ô‡∏≠‡∏Å‡πÉ‡∏à", "‡πÄ‡∏®‡∏£‡πâ‡∏≤‡πÉ‡∏à", "‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á", "‡∏´‡∏°‡∏î‡∏£‡∏±‡∏Å"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô ===
    "financial": ["‡πÄ‡∏á‡∏¥‡∏ô", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô", "‡∏´‡∏ô‡∏µ‡πâ", "‡∏≠‡∏≠‡∏°", "‡∏•‡∏á‡∏ó‡∏∏‡∏ô", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡πÅ‡∏û‡∏á"],
    "shopping": ["‡∏ã‡∏∑‡πâ‡∏≠", "‡∏Ç‡∏≤‡∏¢", "‡∏ä‡πâ‡∏≠‡∏õ", "‡∏•‡∏î", "‡πÇ‡∏õ‡∏£", "‡∏ü‡∏£‡∏µ", "‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î", "‡πÄ‡∏ã‡∏•"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û ===
    "fitness": ["‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á", "‡∏ü‡∏¥‡∏ï", "‡∏¢‡∏¥‡∏°", "‡∏ß‡∏¥‡πà‡∏á", "‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å", "‡∏Å‡∏•‡πâ‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠", "‡πÇ‡∏¢‡∏Ñ‡∏∞"],
    "food": ["‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏Å‡∏¥‡∏ô", "‡∏≠‡∏£‡πà‡∏≠‡∏¢", "‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡πÄ‡∏°‡∏ô‡∏π", "‡∏´‡∏¥‡∏ß"],
    
    # === ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ===
    "travel": ["‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á", "‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°", "‡∏ï‡∏±‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô", "‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß", "‡∏ß‡∏¥‡∏ß"]
}

# === INTENSITY PATTERNS ===
INTENSITY_PATTERNS = {
    "high": ["‡∏°‡∏≤‡∏Å", "‡πÄ‡∏•‡∏¢", "‡∏™‡∏∏‡∏î", "‡πÅ‡∏£‡∏á", "‡∏´‡∏ô‡∏±‡∏Å", "‡πÇ‡∏Ñ‡∏ï‡∏£", "‡πÅ‡∏™‡∏ô", "‡∏™‡∏≤‡∏´‡∏±‡∏™", "‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡πâ‡∏≤", "‡∏à‡∏£‡∏¥‡∏á‡πÜ"],
    "medium": ["‡∏û‡∏≠", "‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á", "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ", "‡πÇ‡∏≠‡πÄ‡∏Ñ"],
    "low": ["‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢", "‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡πÄ‡∏ö‡∏≤‡πÜ", "‡∏ô‡∏¥‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß", "‡πÑ‡∏°‡πà‡∏°‡∏≤‡∏Å"]
}

# --- Helper: Run yt-dlp for a single link ---
def run_ytdlp(link, outtmpl=None):
    video_id = link.split('v=')[-1].split('&')[0]
    # Always save to data/ directory
    infojson = f"data/{video_id}.info.json"
    if os.path.exists(infojson):
        return infojson
    # Ensure data/ directory exists
    if not os.path.exists('data'):
        os.makedirs('data')
    # Always use outtmpl to force yt-dlp to save in data/
    outtmpl = outtmpl or 'data/%(id)s.%(ext)s'
    cmd = [
        'python', '-m', 'yt_dlp',
        '--write-comments',
        '--skip-download',
        link,
        '-o', outtmpl
    ]
    subprocess.run(cmd, check=True)
    # Find the .info.json file in data/
    files = glob(f"data/*{video_id}*.info.json")
    return files[0] if files else None

def mask_author(author):
    if not author:
        return None
    return hashlib.sha256(author.encode('utf-8')).hexdigest()[:12]

def clean_text_privacy(text):
    if not text:
        return text
    # Mask phone numbers
    text = re.sub(r'\b\d{8,}\b', '[MASKED_PHONE]', text)
    # Mask emails
    text = re.sub(r'[\w\.-]+@[\w\.-]+', '[MASKED_EMAIL]', text)
    # Mask URLs
    text = re.sub(r'https?://\S+', '[MASKED_URL]', text)
    text = re.sub(r'www\.\S+', '[MASKED_URL]', text)
    return text

# --- Helper: Recursively flatten comments and replies ---
def flatten_comments(comments, video_id, parent_id=None, privacy_mode='none', sentiment_mode='basic', detailed_mode='single'):
    rows = []
    for c in comments:
        comment_text = c.get('text', '')
        
        # Sentiment analysis based on mode
        if sentiment_mode == 'detailed' and DETAILED_SENTIMENT_AVAILABLE:
            # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö ML (detailed sentiment analysis)
            sentiment_result = analyze_detailed_sentiment(
                comment_text, 
                mode=detailed_mode,  # 'single' ‡∏´‡∏£‡∏∑‡∏≠ 'multi'
                threshold=0.3,
                include_scores=True
            )
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö schema ‡πÄ‡∏î‡∏¥‡∏°
            sentiment_basic = sentiment_result.get('sentiment', 'neutral')
            # confidence: ‡∏ñ‡πâ‡∏≤ ML ‡∏Ñ‡∏∑‡∏ô confidence ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ max score ‡∏Ç‡∏≠‡∏á detailed_emotions
            confidence = sentiment_result.get('confidence')
            if confidence is None:
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ max score ‡∏à‡∏≤‡∏Å all_scores ‡∏Ç‡∏≠‡∏á detailed_emotions
                all_scores = sentiment_result.get('all_scores', {})
                detailed_emotions = sentiment_result.get('detailed_emotions', [])
                if detailed_emotions and all_scores:
                    confs = [all_scores.get(em, 0.0) for em in detailed_emotions if all_scores.get(em, 0.0) > 0.0]
                    confidence = max(confs) if confs else 0.0
                else:
                    confidence = 0.0
            sentiment_score = _sentiment_to_score(sentiment_basic)

        elif sentiment_mode == 'enhanced' and DETAILED_SENTIMENT_AVAILABLE:
            # ‡πÉ‡∏ä‡πâ enhanced analysis (backward compatible)
            sentiment_result = enhanced_analyze_sentiment(comment_text)
            sentiment_basic = sentiment_result.get('sentiment', 'neutral')
            confidence = sentiment_result.get('confidence', 0.0)
            sentiment_score = sentiment_result.get('sentiment_score', 0.0)

        else:
            # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö built-in (pattern matching) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ external modules
            sentiment_result = analyze_sentiment_builtin(
                comment_text, 
                mode=detailed_mode if sentiment_mode == 'detailed' else 'single',
                threshold=0.3
            )
            sentiment_basic = sentiment_result.get('sentiment', 'neutral')
            confidence = sentiment_result.get('confidence', 0.0)
            sentiment_score = sentiment_result.get('sentiment_score', 0.0)
        
        # Privacy handling
        author = c.get('author')
        if privacy_mode == 'mask':
            author = mask_author(author)
        elif privacy_mode == 'remove':
            author = None
        text = c.get('text')
        if privacy_mode in ('mask', 'remove'):
            text = clean_text_privacy(text)
        
        # One-hot encoding for pos/neu/neg/other
        pos = 1 if sentiment_basic == 'positive' else 0
        neu = 1 if sentiment_basic == 'neutral' else 0
        neg = 1 if sentiment_basic == 'negative' else 0
        other = 1 if sentiment_basic not in ('positive', 'neutral', 'negative') else 0
        # Base row structure
        row = {
            'video_id': video_id,
            'comment_id': c.get('id'),
            'parent_id': parent_id,
            'author': author,
            'text': text,
            'like_count': c.get('like_count'),
            'published': c.get('published'),
            'is_reply': parent_id is not None,
            # --- ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô sentiment schema ---
            'sentiment': sentiment_basic,
            'confidence': confidence,
            'sentiment_score': sentiment_score,
            'pos': pos,
            'neu': neu,
            'neg': neg,
            'other': other,
            'model_type': sentiment_result.get('model_type', 'unknown'),
            'privacy_notice': 'This dataset is for research only. Do not use for commercial or personal identification.'
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• detailed sentiment ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        if sentiment_mode == 'detailed':
            if DETAILED_SENTIMENT_AVAILABLE:
                # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å external detailed sentiment module
                row.update({
                    'detailed_sentiment_analysis': sentiment_result,
                    'analysis_mode': detailed_mode,
                    'detailed_emotions': sentiment_result.get('detailed_emotions', []) if detailed_mode == 'multi' else [sentiment_result.get('detailed_emotion', '')],
                    'emotion_groups': sentiment_result.get('emotion_groups', []) if detailed_mode == 'multi' else [sentiment_result.get('emotion_group', '')],
                    'context': sentiment_result.get('context', 'unknown')
                })
            else:
                # ‡πÉ‡∏ä‡πâ built-in patterns
                if detailed_mode == 'multi':
                    row.update({
                        'detailed_sentiment_analysis': sentiment_result,
                        'analysis_mode': detailed_mode,
                        'detailed_emotions': sentiment_result.get('detailed_emotions', []),
                        'emotion_groups': sentiment_result.get('emotion_groups', []),
                        'context': sentiment_result.get('context', 'unknown')
                    })
                else:
                    row.update({
                        'detailed_sentiment_analysis': sentiment_result,
                        'analysis_mode': detailed_mode,
                        'detailed_emotions': [sentiment_result.get('detailed_emotion', '')],
                        'emotion_groups': [sentiment_result.get('emotion_group', '')],
                        'context': sentiment_result.get('context', 'unknown')
                    })
        
        elif sentiment_mode == 'enhanced':
            if DETAILED_SENTIMENT_AVAILABLE:
                # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å external enhanced module
                row.update({
                    'detailed_emotion': sentiment_result.get('detailed_emotion', ''),
                    'emotion_group': sentiment_result.get('emotion_group', ''),
                    'context': sentiment_result.get('context', 'unknown')
                })
            else:
                # ‡πÉ‡∏ä‡πâ built-in patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö enhanced mode
                row.update({
                    'detailed_emotion': sentiment_result.get('detailed_emotion', ''),
                    'emotion_group': sentiment_result.get('emotion_group', ''),
                    'context': sentiment_result.get('context', 'unknown')
                })
        
        rows.append(row)
        
        # Recursively add replies
        replies = c.get('replies', [])
        if replies:
            rows.extend(flatten_comments(replies, video_id, parent_id=c.get('id'), privacy_mode=privacy_mode, sentiment_mode=sentiment_mode, detailed_mode=detailed_mode))
    return rows

def _sentiment_to_score(sentiment: str) -> float:
    """‡πÅ‡∏õ‡∏•‡∏á sentiment ‡πÄ‡∏õ‡πá‡∏ô score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backward compatibility"""
    if sentiment == "positive":
        return 0.7
    elif sentiment == "negative":
        return -0.7
    else:
        return 0.0

# === BUILT-IN SENTIMENT ANALYSIS FUNCTIONS ===

def extract_emojis(text):
    """‡∏î‡∏∂‡∏á emojis ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map
        "\U0001F1E0-\U0001F1FF"  # flags
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE
    )
    return emoji_pattern.findall(text)

def calculate_emotion_scores(text):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå"""
    if not text:
        return {}
    
    clean_text = text.lower()
    emojis = extract_emojis(text)
    emotion_scores = {}
    
    for emotion, config in EMOTION_PATTERNS.items():
        score = 0.0
        
        # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        for keyword in config["keywords"]:
            if keyword in clean_text:
                score += 1.0
        
        # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å patterns (regex)
        for pattern in config.get("patterns", []):
            matches = re.findall(pattern, clean_text)
            score += len(matches) * 1.5
        
        # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å emojis
        for emoji in emojis:
            if emoji in config.get("emojis", []):
                score += 2.0
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô
        intensity_bonus = calculate_intensity_bonus(clean_text)
        score *= (1 + intensity_bonus)
        
        emotion_scores[emotion] = min(score, 5.0)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    
    return emotion_scores

def calculate_intensity_bonus(text):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì bonus ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≠‡∏Å"""
    bonus = 0.0
    
    for intensity, words in INTENSITY_PATTERNS.items():
        for word in words:
            if word in text:
                if intensity == "high":
                    bonus += 0.5
                elif intensity == "medium":
                    bonus += 0.2
                elif intensity == "low":
                    bonus += 0.1
    
    return min(bonus, 1.0)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î bonus ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î

def normalize_scores(scores):
    """normalize ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1"""
    if not scores:
        return {}
    
    max_score = max(scores.values())
    if max_score == 0:
        return scores
    
    return {emotion: score / max_score for emotion, score in scores.items()}

def determine_context(text):
    """‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    if not text:
        return {
            "primary_context": "neutral",
            "all_contexts": {},
            "formality_level": "neutral",
            "social_setting": "general",
            "emotional_tone": "neutral",
            "generation": "unknown",
            "profession": "general"
        }
    
    clean_text = text.lower()
    context_scores = {}
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
    for context, words in CONTEXT_PATTERNS.items():
        score = 0
        for word in words:
            if word in clean_text:
                score += 1
        if score > 0:
            context_scores[context] = score
    
    if not context_scores:
        return {
            "primary_context": "neutral",
            "all_contexts": {},
            "formality_level": "neutral",
            "social_setting": "general",
            "emotional_tone": "neutral",
            "generation": "unknown",
            "profession": "general"
        }
    
    # ‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å
    primary_context = max(context_scores, key=context_scores.get)
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
    formality_score = {
        "formal": context_scores.get("formal", 0),
        "informal": context_scores.get("informal", 0),
        "slang": context_scores.get("slang", 0)
    }
    formality_level = max(formality_score, key=formality_score.get) if any(formality_score.values()) else "neutral"
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°
    social_contexts = ["social_media", "news_media", "review", "business", "education", "healthcare", "government"]
    social_scores = {ctx: context_scores.get(ctx, 0) for ctx in social_contexts}
    social_setting = max(social_scores, key=social_scores.get) if any(social_scores.values()) else "general"
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏ó‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
    emotional_contexts = ["complaint", "praise", "emergency", "celebration", "condolence", "question"]
    emotional_scores = {ctx: context_scores.get(ctx, 0) for ctx in emotional_contexts}
    emotional_tone = max(emotional_scores, key=emotional_scores.get) if any(emotional_scores.values()) else "neutral"
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∏‡πà‡∏ô/‡∏≠‡∏≤‡∏¢‡∏∏
    generation_contexts = ["gen_z", "millennial", "gen_x"]
    generation_scores = {ctx: context_scores.get(ctx, 0) for ctx in generation_contexts}
    generation = max(generation_scores, key=generation_scores.get) if any(generation_scores.values()) else "unknown"
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û
    profession_contexts = ["business", "education", "healthcare", "government", "tech", "gaming"]
    profession_scores = {ctx: context_scores.get(ctx, 0) for ctx in profession_contexts}
    profession = max(profession_scores, key=profession_scores.get) if any(profession_scores.values()) else "general"
    
    return {
        "primary_context": primary_context,
        "all_contexts": context_scores,
        "formality_level": formality_level,
        "social_setting": social_setting,
        "emotional_tone": emotional_tone,
        "generation": generation,
        "profession": profession,
        "context_confidence": round(context_scores[primary_context] / len(clean_text.split()) if clean_text.split() else 0, 3)
    }

def analyze_sentiment_builtin(text, mode='single', threshold=0.3):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡πÅ‡∏ö‡∏ö built-in (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô)"""
    if not text or not text.strip():
        return {
            "sentiment": "neutral",
            "confidence": 0.0,
            "sentiment_score": 0.0,
            "detailed_emotion": "‡πÄ‡∏â‡∏¢ ‡πÜ",
            "emotion_group": "Neutral",
            "context": {
                "primary_context": "neutral",
                "formality_level": "neutral",
                "social_setting": "general",
                "emotional_tone": "neutral"
            },
            "model_type": "builtin_pattern_matching"
        }
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
    raw_scores = calculate_emotion_scores(text)
    normalized_scores = normalize_scores(raw_scores)
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
    context_data = determine_context(text)
    context_patterns = analyze_context_patterns(text)
    context_insights = get_context_insights(context_data)
    
    if mode == 'single':
        # Single label mode
        if not normalized_scores or max(normalized_scores.values(), default=0.0) == 0.0:
            predicted_label = "‡πÄ‡∏â‡∏¢ ‡πÜ"
            confidence = 0.0
        else:
            predicted_label = max(normalized_scores, key=normalized_scores.get)
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô 0.0 ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô neutral
            if normalized_scores[predicted_label] == 0.0:
                predicted_label = "‡πÄ‡∏â‡∏¢ ‡πÜ"
                confidence = 0.0
            else:
                confidence = normalized_scores[predicted_label]

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞ sentiment ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        group = LABEL_TO_GROUP.get(predicted_label, "Neutral")
        basic_sentiment = "positive" if group == "Positive" else "negative" if group == "Negative" else "neutral"
        sentiment_score = _sentiment_to_score(basic_sentiment)

        result = {
            "sentiment": basic_sentiment,
            "confidence": round(confidence, 3),
            "sentiment_score": sentiment_score,
            "detailed_emotion": predicted_label,
            "emotion_group": group,
            "context": context_data,
            "context_patterns": context_patterns,
            "context_insights": context_insights,
            "scores": {k: round(v, 3) for k, v in normalized_scores.items()},
            "model_type": "builtin_pattern_matching"
        }

    else:  # multi label mode
        # Multi-label mode
        predicted_labels = []
        for emotion, score in normalized_scores.items():
            if score >= threshold:
                predicted_labels.append(emotion)

        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ label ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô threshold ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 0.0 ‡πÉ‡∏´‡πâ default ‡πÄ‡∏õ‡πá‡∏ô "‡πÄ‡∏â‡∏¢ ‡πÜ"
        if (not predicted_labels) or all(normalized_scores.get(label, 0.0) == 0.0 for label in predicted_labels):
            predicted_labels = ["‡πÄ‡∏â‡∏¢ ‡πÜ"]

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞ sentiment ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        groups = list(set(LABEL_TO_GROUP.get(label, "Neutral") for label in predicted_labels))

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î basic sentiment ‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å
        if "Positive" in groups:
            basic_sentiment = "positive"
        elif "Negative" in groups:
            basic_sentiment = "negative"
        else:
            basic_sentiment = "neutral"

        sentiment_score = _sentiment_to_score(basic_sentiment)
        # confidence = max ‡∏Ç‡∏≠‡∏á label ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 0.0 ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        nonzero_conf = [normalized_scores.get(label, 0.0) for label in predicted_labels if normalized_scores.get(label, 0.0) > 0.0]
        max_confidence = max(nonzero_conf) if nonzero_conf else 0.0

        result = {
            "sentiment": basic_sentiment,
            "confidence": round(max_confidence, 3),
            "sentiment_score": sentiment_score,
            "detailed_emotions": predicted_labels,
            "emotion_groups": groups,
            "context": context_data,
            "context_patterns": context_patterns,
            "context_insights": context_insights,
            "scores": {k: round(v, 3) for k, v in normalized_scores.items()},
            "threshold": threshold,
            "model_type": "builtin_pattern_matching"
        }

    return result

# --- Main ---

# --- Move show_sentiment_statistics above main() to avoid NameError ---
def show_sentiment_statistics(rows, detailed_mode):
    """Show summary statistics for sentiment analysis"""
    if not rows:
        return
    print(f"\nüìä Sentiment Analysis Statistics:")
    print(f"   Total comments analyzed: {len(rows)}")
    print(f"   Analysis mode: {detailed_mode}")
    # Basic sentiment counts
    sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
    emotion_counts = {}
    group_counts = {}
    context_counts = {
        "primary_context": {},
        "formality_level": {},
        "social_setting": {},
        "emotional_tone": {},
        "generation": {},
        "profession": {}
    }
    for row in rows:
        # Basic sentiment
        sentiment = row.get('sentiment', 'neutral')
        if sentiment in sentiment_counts:
            sentiment_counts[sentiment] += 1
        # Detailed emotions (if available)
        if detailed_mode == 'single':
            emotions = row.get('detailed_emotions', [])
            if not emotions:
                emotion = row.get('detailed_emotion', '')
                emotions = [emotion] if emotion else ['']
            groups = row.get('emotion_groups', [])
            if not groups:
                group = row.get('emotion_group', '')
                groups = [group] if group else ['']
        else:  # multi
            emotions = row.get('detailed_emotions', [])
            groups = row.get('emotion_groups', [])
        for emotion in emotions:
            if emotion and emotion.strip():
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        for group in groups:
            if group and group.strip():
                group_counts[group] = group_counts.get(group, 0) + 1
        # Context analysis (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà)
        context = row.get('context', {})
        if isinstance(context, dict):
            for context_type in context_counts:
                if context_type in context:
                    value = context[context_type]
                    if value and str(value).strip():
                        context_counts[context_type][value] = context_counts[context_type].get(value, 0) + 1
        elif isinstance(context, str):
            if context and context.strip():
                context_counts["primary_context"][context] = context_counts["primary_context"].get(context, 0) + 1
    print(f"   Basic sentiment distribution:")
    for sentiment, count in sentiment_counts.items():
        percentage = (count / len(rows)) * 100
        print(f"     {sentiment}: {count} ({percentage:.1f}%)")
    if emotion_counts:
        print(f"   Top emotions detected:")
        sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:8]
        for emotion, count in sorted_emotions:
            percentage = (count / len(rows)) * 100
            print(f"     {emotion}: {count} ({percentage:.1f}%)")
    if group_counts:
        print(f"   Emotion group distribution:")
        for group, count in group_counts.items():
            percentage = (count / len(rows)) * 100
            print(f"     {group}: {count} ({percentage:.1f}%)")
    print(f"   üìã Context Analysis:")
    if context_counts["formality_level"]:
        print(f"     Formality levels:")
        for level, count in context_counts["formality_level"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {level}: {count} ({percentage:.1f}%)")
    if context_counts["social_setting"]:
        print(f"     Social settings:")
        sorted_social = sorted(context_counts["social_setting"].items(), key=lambda x: x[1], reverse=True)[:5]
        for setting, count in sorted_social:
            percentage = (count / len(rows)) * 100
            print(f"       {setting}: {count} ({percentage:.1f}%)")
    if context_counts["emotional_tone"]:
        print(f"     Emotional tones:")
        sorted_tones = sorted(context_counts["emotional_tone"].items(), key=lambda x: x[1], reverse=True)[:5]
        for tone, count in sorted_tones:
            percentage = (count / len(rows)) * 100
            print(f"       {tone}: {count} ({percentage:.1f}%)")
    if context_counts["generation"]:
        print(f"     Generational markers:")
        for generation, count in context_counts["generation"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {generation}: {count} ({percentage:.1f}%)")
    if context_counts["profession"]:
        print(f"     Professional contexts:")
        sorted_professions = sorted(context_counts["profession"].items(), key=lambda x: x[1], reverse=True)[:5]
        for profession, count in sorted_professions:
            percentage = (count / len(rows)) * 100
            print(f"       {profession}: {count} ({percentage:.1f}%)")
    if context_counts["primary_context"]:
        print(f"     Primary contexts:")
        sorted_contexts = sorted(context_counts["primary_context"].items(), key=lambda x: x[1], reverse=True)[:5]
        for context, count in sorted_contexts:
            percentage = (count / len(rows)) * 100
            print(f"       {context}: {count} ({percentage:.1f}%)")
    model_types = {}
    for row in rows:
        model_type = row.get('model_type', 'unknown')
        model_types[model_type] = model_types.get(model_type, 0) + 1
    if model_types:
        print(f"   Model types used:")
        for model, count in model_types.items():
            percentage = (count / len(rows)) * 100
            print(f"     {model}: {count} ({percentage:.1f}%)")
    print()

def main():
    parser = argparse.ArgumentParser(description='Extract YouTube comments using yt-dlp and export as JSONL with advanced Thai sentiment analysis.')
    parser.add_argument('--links', default='youtube_real_links_1500.txt', help='Text file with YouTube links (one per line)')
    parser.add_argument('--output', default='youtube_comments.jsonl', help='Output JSONL file')
    parser.add_argument('--privacy', default='none', choices=['none', 'mask', 'remove'], help='Privacy mode: mask (hash author, mask PII), remove (no author, mask PII), none (no privacy)')
    # Sentiment analysis options
    parser.add_argument('--sentiment-mode', default='basic', 
                       choices=['basic', 'enhanced', 'detailed'], 
                       help='Sentiment analysis mode: basic (old system), enhanced (new system, backward compatible), detailed (full multi-emotion analysis)')
    parser.add_argument('--detailed-mode', default='single', 
                       choices=['single', 'multi'],
                       help='For detailed sentiment: single (single-label classification) or multi (multi-label classification)')
    parser.add_argument('--no-sentiment', action='store_true', 
                       help='Disable sentiment analysis completely')
    import sys
    if len(sys.argv) == 1:
        parser.print_help()
        print("\n[ERROR] ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ arguments ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏ä‡πà‡∏ô --links <‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏¥‡∏á‡∏Å‡πå YouTube> ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô\n")
        sys.exit(1)
    try:
        args = parser.parse_args()
    except SystemExit as e:
        # argparse throws SystemExit on error, catch and print friendlier message
        if e.code != 0:
            print("\n[ERROR] ‡∏û‡∏ö argument ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ argument ‡πÅ‡∏õ‡∏•‡∏Å‡∏õ‡∏•‡∏≠‡∏°\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô\n")
        raise
    # --- Main processing logic ---
    # 1. Load YouTube links
    try:
        with open(args.links, encoding='utf-8') as f:
            links = [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏¥‡∏á‡∏Å‡πå: {args.links} : {e}")
        return
    # --- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏°‡∏î: ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå .info.json ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏´‡∏£‡∏∑‡∏≠ process ‡πÉ‡∏´‡∏°‡πà ---
    all_rows = []
    for link in tqdm(links, desc="Processing YouTube links"):
        try:
            # ‡∏î‡∏∂‡∏á video_id ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå
            video_id = link.split('v=')[-1].split('&')[0]
            # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå .info.json ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô data/ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            infojson_candidates = [
                f"data/{video_id}.info.json",
                f"{video_id}.info.json"
            ]
            infojson = None
            for candidate in infojson_candidates:
                if os.path.exists(candidate):
                    infojson = candidate
                    break
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô yt-dlp ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà
            if not infojson:
                infojson = run_ytdlp(link)
                if not infojson or not os.path.exists(infojson):
                    print(f"[WARN] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {link}")
                    continue
            with open(infojson, encoding='utf-8') as jf:
                info = json.load(jf)
            comments = info.get('comments', [])
            video_id = info.get('id', video_id)
            if args.no_sentiment:
                rows = flatten_comments_no_sentiment(comments, video_id, privacy_mode=args.privacy)
            else:
                rows = flatten_comments(
                    comments, video_id,
                    privacy_mode=args.privacy,
                    sentiment_mode=args.sentiment_mode,
                    detailed_mode=args.detailed_mode
                )
            all_rows.extend(rows)
        except Exception as e:
            print(f"[ERROR] ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏ó‡∏µ‡πà‡∏•‡∏¥‡∏á‡∏Å‡πå {link}: {e}")
    # 4. Write output
    try:
        with open(args.output, 'w', encoding='utf-8') as out:
            for row in all_rows:
                out.write(json.dumps(row, ensure_ascii=False) + '\n')
        print(f"\n[INFO] ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà {args.output} ‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(all_rows)} records")
    except Exception as e:
        print(f"[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå output: {args.output} : {e}")
        return
    # 5. Show statistics (optional)
    if not args.no_sentiment:
        show_sentiment_statistics(all_rows, args.detailed_mode)

# --- Main entry point ---
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[FATAL ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÑ‡∏î‡πâ: ", e)
        import sys
        sys.exit(1)

def flatten_comments_no_sentiment(comments, video_id, parent_id=None, privacy_mode='none'):
    """Flatten comments without sentiment analysis (for performance/testing)"""
    rows = []
    for c in comments:
        # Privacy handling
        author = c.get('author')
        if privacy_mode == 'mask':
            author = mask_author(author)
        elif privacy_mode == 'remove':
            author = None
        text = c.get('text')
        if privacy_mode in ('mask', 'remove'):
            text = clean_text_privacy(text)
        
        # Default: all 0, only neu=1 (since no sentiment analysis)
        row = {
            'video_id': video_id,
            'comment_id': c.get('id'),
            'parent_id': parent_id,
            'author': author,
            'text': text,
            'like_count': c.get('like_count'),
            'published': c.get('published'),
            'is_reply': parent_id is not None,
            'pos': 0,
            'neu': 1,
            'neg': 0,
            'privacy_notice': 'This dataset is for research only. Do not use for commercial or personal identification.'
        }
        rows.append(row)
        
        # Recursively add replies
        replies = c.get('replies', [])
        if replies:
            rows.extend(flatten_comments_no_sentiment(replies, video_id, parent_id=c.get('id'), privacy_mode=privacy_mode))
    return rows

def show_sentiment_statistics(rows, detailed_mode):
    """Show summary statistics for sentiment analysis"""
    if not rows:
        return
    
    print(f"\nüìä Sentiment Analysis Statistics:")
    print(f"   Total comments analyzed: {len(rows)}")
    print(f"   Analysis mode: {detailed_mode}")
    
    # Basic sentiment counts
    sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
    emotion_counts = {}
    group_counts = {}
    context_counts = {
        "primary_context": {},
        "formality_level": {},
        "social_setting": {},
        "emotional_tone": {},
        "generation": {},
        "profession": {}
    }
    
    for row in rows:
        # Basic sentiment
        sentiment = row.get('sentiment', 'neutral')
        if sentiment in sentiment_counts:
            sentiment_counts[sentiment] += 1
        
        # Detailed emotions (if available)
        if detailed_mode == 'single':
            # ‡∏î‡∏∂‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å detailed_emotions array ‡∏´‡∏£‡∏∑‡∏≠ detailed_emotion field
            emotions = row.get('detailed_emotions', [])
            if not emotions:
                emotion = row.get('detailed_emotion', '')
                emotions = [emotion] if emotion else ['']
            
            groups = row.get('emotion_groups', [])
            if not groups:
                group = row.get('emotion_group', '')
                groups = [group] if group else ['']
        else:  # multi
            emotions = row.get('detailed_emotions', [])
            groups = row.get('emotion_groups', [])
        
        for emotion in emotions:
            if emotion and emotion.strip():
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        for group in groups:
            if group and group.strip():
                group_counts[group] = group_counts.get(group, 0) + 1
        
        # Context analysis (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà)
        context = row.get('context', {})
        if isinstance(context, dict):
            # ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            for context_type in context_counts:
                if context_type in context:
                    value = context[context_type]
                    if value and str(value).strip():
                        context_counts[context_type][value] = context_counts[context_type].get(value, 0) + 1
        elif isinstance(context, str):
            # ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤
            if context and context.strip():
                context_counts["primary_context"][context] = context_counts["primary_context"].get(context, 0) + 1
    
    print(f"   Basic sentiment distribution:")
    for sentiment, count in sentiment_counts.items():
        percentage = (count / len(rows)) * 100
        print(f"     {sentiment}: {count} ({percentage:.1f}%)")
    
    if emotion_counts:
        print(f"   Top emotions detected:")
        sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:8]
        for emotion, count in sorted_emotions:
            percentage = (count / len(rows)) * 100
            print(f"     {emotion}: {count} ({percentage:.1f}%)")
    
    if group_counts:
        print(f"   Emotion group distribution:")
        for group, count in group_counts.items():
            percentage = (count / len(rows)) * 100
            print(f"     {group}: {count} ({percentage:.1f}%)")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    print(f"   üìã Context Analysis:")
    
    # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
    if context_counts["formality_level"]:
        print(f"     Formality levels:")
        for level, count in context_counts["formality_level"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {level}: {count} ({percentage:.1f}%)")
    
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°
    if context_counts["social_setting"]:
        print(f"     Social settings:")
        sorted_social = sorted(context_counts["social_setting"].items(), key=lambda x: x[1], reverse=True)[:5]
        for setting, count in sorted_social:
            percentage = (count / len(rows)) * 100
            print(f"       {setting}: {count} ({percentage:.1f}%)")
    
    # ‡πÇ‡∏ó‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
    if context_counts["emotional_tone"]:
        print(f"     Emotional tones:")
        sorted_tones = sorted(context_counts["emotional_tone"].items(), key=lambda x: x[1], reverse=True)[:5]
        for tone, count in sorted_tones:
            percentage = (count / len(rows)) * 100
            print(f"       {tone}: {count} ({percentage:.1f}%)")
    
    # ‡∏£‡∏∏‡πà‡∏ô/‡∏≠‡∏≤‡∏¢‡∏∏
    if context_counts["generation"]:
        print(f"     Generational markers:")
        for generation, count in context_counts["generation"].items():
            percentage = (count / len(rows)) * 100
            print(f"       {generation}: {count} ({percentage:.1f}%)")
    
    # ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û
    if context_counts["profession"]:
        print(f"     Professional contexts:")
        sorted_professions = sorted(context_counts["profession"].items(), key=lambda x: x[1], reverse=True)[:5]
        for profession, count in sorted_professions:
            percentage = (count / len(rows)) * 100
            print(f"       {profession}: {count} ({percentage:.1f}%)")
    
    # ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å (fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤)
    if context_counts["primary_context"]:
        print(f"     Primary contexts:")
        sorted_contexts = sorted(context_counts["primary_context"].items(), key=lambda x: x[1], reverse=True)[:5]
        for context, count in sorted_contexts:
            percentage = (count / len(rows)) * 100
            print(f"       {context}: {count} ({percentage:.1f}%)")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö model ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
    model_types = {}
    for row in rows:
        model_type = row.get('model_type', 'unknown')
        model_types[model_type] = model_types.get(model_type, 0) + 1
    
    if model_types:
        print(f"   Model types used:")
        for model, count in model_types.items():
            percentage = (count / len(rows)) * 100
            print(f"     {model}: {count} ({percentage:.1f}%)")
    
    print()

def analyze_context_patterns(text):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å"""
    if not text:
        return {}
    
    clean_text = text.lower()
    patterns = {
        "communication_style": {},
        "relationship_level": {},
        "topic_category": {},
        "cultural_elements": {},
        "generational_markers": {},
        "emotional_indicators": {}
    }
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£
    comm_styles = {
        "formal": CONTEXT_PATTERNS["formal"],
        "informal": CONTEXT_PATTERNS["informal"], 
        "slang": CONTEXT_PATTERNS["slang"]
    }
    
    for style, words in comm_styles.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["communication_style"][style] = count
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå
    relationship_levels = {
        "intimate": CONTEXT_PATTERNS["intimate"],
        "friendly": CONTEXT_PATTERNS["friendly"],
        "personal": CONTEXT_PATTERNS["personal"]
    }
    
    for level, words in relationship_levels.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["relationship_level"][level] = count
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
    topic_categories = {
        "technology": CONTEXT_PATTERNS.get("tech", []) + CONTEXT_PATTERNS.get("gaming", []),
        "entertainment": CONTEXT_PATTERNS.get("music", []) + CONTEXT_PATTERNS.get("movie", []),
        "lifestyle": CONTEXT_PATTERNS.get("food", []) + CONTEXT_PATTERNS.get("travel", []),
        "business": CONTEXT_PATTERNS.get("business", []) + CONTEXT_PATTERNS.get("financial", [])
    }
    
    for category, words in topic_categories.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["topic_category"][category] = count
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°
    cultural_elements = {
        "traditional": CONTEXT_PATTERNS["traditional"],
        "religious": CONTEXT_PATTERNS["religious"],
        "regional": CONTEXT_PATTERNS["northern"] + CONTEXT_PATTERNS["southern"] + CONTEXT_PATTERNS["northeastern"]
    }
    
    for element, words in cultural_elements.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["cultural_elements"][element] = count
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∏‡πà‡∏ô
    generational_markers = {
        "gen_z": CONTEXT_PATTERNS["gen_z"],
        "millennial": CONTEXT_PATTERNS["millennial"],
        "gen_x": CONTEXT_PATTERNS["gen_x"]
    }
    
    for generation, words in generational_markers.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["generational_markers"][generation] = count
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏±‡∏ß‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
    emotional_indicators = {
        "positive_tone": CONTEXT_PATTERNS["praise"] + CONTEXT_PATTERNS["celebration"],
        "negative_tone": CONTEXT_PATTERNS["complaint"] + CONTEXT_PATTERNS["emergency"],
        "questioning": CONTEXT_PATTERNS["question"]
    }
    
    for indicator, words in emotional_indicators.items():
        count = sum(1 for word in words if word in clean_text)
        if count > 0:
            patterns["emotional_indicators"][indicator] = count
    
    return patterns

def get_context_insights(context_data):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á insights ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó"""
    if not isinstance(context_data, dict):
        return {}
    
    insights = {
        "communication_appropriateness": "",
        "audience_recommendation": "",
        "tone_analysis": "",
        "cultural_sensitivity": "",
        "generational_appeal": ""
    }
    
    formality = context_data.get("formality_level", "neutral")
    social_setting = context_data.get("social_setting", "general")
    emotional_tone = context_data.get("emotional_tone", "neutral")
    generation = context_data.get("generation", "unknown")
    profession = context_data.get("profession", "general")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£
    if formality == "slang" and social_setting in ["business", "government", "education"]:
        insights["communication_appropriateness"] = "‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°: ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏™‡πÅ‡∏•‡∏á‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"
    elif formality == "formal" and social_setting == "social_media":
        insights["communication_appropriateness"] = "‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ: ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"
    else:
        insights["communication_appropriateness"] = "‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó"
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    if generation != "unknown":
        insights["audience_recommendation"] = f"‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏° {generation}"
    else:
        insights["audience_recommendation"] = "‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏¢‡∏∏"
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏ó‡∏ô
    if emotional_tone in ["complaint", "emergency"]:
        insights["tone_analysis"] = "‡πÇ‡∏ó‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö: ‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤"
    elif emotional_tone in ["praise", "celebration"]:
        insights["tone_analysis"] = "‡πÇ‡∏ó‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å: ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ"
    else:
        insights["tone_analysis"] = "‡πÇ‡∏ó‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á: ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°
    if profession in ["government", "education", "healthcare"]:
        insights["cultural_sensitivity"] = "‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏•‡πà‡∏ß‡∏á‡πÄ‡∏Å‡∏¥‡∏ô"
    else:
        insights["cultural_sensitivity"] = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏û‡∏¥‡πÄ‡∏®‡∏©"
    
    # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏î‡∏π‡∏î‡∏ï‡∏≤‡∏°‡∏£‡∏∏‡πà‡∏ô
    if generation == "gen_z":
        insights["generational_appeal"] = "‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà"
    elif generation == "millennial":
        insights["generational_appeal"] = "‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á"
    elif generation == "gen_x":
        insights["generational_appeal"] = "‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
    else:
        insights["generational_appeal"] = "‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏¢‡∏∏"
    
    return insights
